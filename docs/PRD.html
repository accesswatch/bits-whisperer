<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>BITS Whisperer — Product Requirements Document</title>
<link rel="stylesheet" href="docs.css">
</head>
<body>
<nav class="toc"><strong>Table of Contents</strong>
<div class="toc">
<ul>
<li><a href="#bits-whisperer-product-requirements-document">BITS Whisperer — Product Requirements Document</a><ul>
<li><a href="#1-purpose-vision">1. Purpose &amp; Vision</a><ul>
<li><a href="#design-pillars">Design Pillars</a></li>
</ul>
</li>
<li><a href="#2-target-users">2. Target Users</a></li>
<li><a href="#3-whisper-models-local-inference">3. Whisper Models (Local Inference)</a></li>
<li><a href="#4-transcription-providers">4. Transcription Providers</a><ul>
<li><a href="#provider-selection">Provider Selection</a></li>
<li><a href="#cloud-provider-onboarding">Cloud Provider Onboarding</a></li>
<li><a href="#auphonic-integration">Auphonic Integration</a></li>
<li><a href="#provider-specific-settings">Provider-Specific Settings</a></li>
<li><a href="#ai-services-translation-summarization-chat">AI Services (Translation, Summarization &amp; Chat)</a></li>
<li><a href="#ai-model-catalog">AI Model Catalog</a></li>
<li><a href="#copilot-subscription-tiers">Copilot Subscription Tiers</a></li>
<li><a href="#prompt-templates">Prompt Templates</a></li>
<li><a href="#custom-vocabulary">Custom Vocabulary</a></li>
<li><a href="#multi-language-simultaneous-translation">Multi-Language Simultaneous Translation</a></li>
<li><a href="#real-time-streaming-transcription">Real-Time Streaming Transcription</a></li>
<li><a href="#github-copilot-sdk-integration">GitHub Copilot SDK Integration</a></li>
<li><a href="#ai-actions-post-transcription-processing">AI Actions (Post-Transcription Processing)</a></li>
<li><a href="#context-window-management">Context Window Management</a></li>
<li><a href="#speaker-diarization">Speaker Diarization</a></li>
</ul>
</li>
<li><a href="#5-audio-preprocessing-pipeline">5. Audio Preprocessing Pipeline</a></li>
<li><a href="#6-audio-format-support">6. Audio Format Support</a></li>
<li><a href="#7-batch-folder-processing">7. Batch &amp; Folder Processing</a><ul>
<li><a href="#budget-limits">Budget Limits</a></li>
<li><a href="#cost-estimation">Cost Estimation</a></li>
<li><a href="#queue-panel-uiqueue_panelpy">Queue Panel (ui/queue_panel.py)</a></li>
<li><a href="#custom-job-naming">Custom Job Naming</a></li>
</ul>
</li>
<li><a href="#8-background-processing-system-tray">8. Background Processing &amp; System Tray</a><ul>
<li><a href="#system-tray-icon-uitray_iconpy">System Tray Icon (ui/tray_icon.py)</a></li>
<li><a href="#minimize-to-tray">Minimize to Tray</a></li>
<li><a href="#notifications">Notifications</a></li>
</ul>
</li>
<li><a href="#9-output-export">9. Output &amp; Export</a><ul>
<li><a href="#7-export-formats">7 Export Formats</a></li>
<li><a href="#auto-export-on-completion">Auto-Export on Completion</a></li>
</ul>
</li>
<li><a href="#10-user-interface">10. User Interface</a><ul>
<li><a href="#layout">Layout</a></li>
<li><a href="#menu-structure">Menu Structure</a></li>
<li><a href="#settings-dialog-9-tabs">Settings Dialog (9 tabs)</a></li>
<li><a href="#simple-vs-advanced-mode">Simple vs Advanced Mode</a></li>
<li><a href="#recent-files">Recent Files</a></li>
</ul>
</li>
<li><a href="#11-self-update-system">11. Self-Update System</a></li>
<li><a href="#12-architecture">12. Architecture</a><ul>
<li><a href="#file-tree">File Tree</a></li>
<li><a href="#threading-model">Threading Model</a></li>
<li><a href="#shutdown-temp-file-cleanup">Shutdown &amp; Temp File Cleanup</a></li>
</ul>
</li>
<li><a href="#13-data-model">13. Data Model</a><ul>
<li><a href="#job-corejobpy">Job (core/job.py)</a></li>
<li><a href="#transcriptionresult">TranscriptionResult</a></li>
<li><a href="#persistence">Persistence</a></li>
</ul>
</li>
<li><a href="#14-privacy-security">14. Privacy &amp; Security</a></li>
<li><a href="#15-accessibility-requirements-non-negotiable">15. Accessibility Requirements (Non-Negotiable)</a></li>
<li><a href="#16-keyboard-shortcuts">16. Keyboard Shortcuts</a></li>
<li><a href="#17-paths-directories">17. Paths &amp; Directories</a></li>
<li><a href="#18-dependencies">18. Dependencies</a></li>
<li><a href="#19-implementation-status">19. Implementation Status</a></li>
<li><a href="#20-success-metrics">20. Success Metrics</a></li>
<li><a href="#21-decisions-log">21. Decisions Log</a></li>
</ul>
</li>
</ul>
</div>
</nav>
<h1 id="bits-whisperer-product-requirements-document">BITS Whisperer — Product Requirements Document</h1>
<blockquote>
<p><strong>Version:</strong> 1.0.0 - <strong>Updated:</strong> 2026-02-08 - <strong>Status:</strong> Implementation
Complete</p>
<p>Developed by <strong>Blind Information Technology Solutions (BITS)</strong></p>
</blockquote>
<hr>
<h2 id="1-purpose-vision">1. Purpose &amp; Vision</h2>
<p><strong>BITS Whisperer</strong> is a consumer-grade WXPython desktop application for audio
transcription. It targets Windows and macOS users who need reliable
speech-to-text without technical expertise — journalists, students, researchers,
accessibility advocates, and content creators.</p>
<h3 id="design-pillars">Design Pillars</h3>
<table>
<thead>
<tr>
<th>Pillar</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Accessible</strong></td>
<td>WCAG 2.1/2.2 adapted for desktop; menu bar primary interface; full keyboard + screen reader support</td>
</tr>
<tr>
<td><strong>Private</strong></td>
<td>Local transcript storage by default; API keys in OS credential store; offline-capable providers</td>
</tr>
<tr>
<td><strong>Versatile</strong></td>
<td>17 transcription providers (cloud + local); 14 Whisper models; 7 export formats; Auphonic audio post-production</td>
</tr>
<tr>
<td><strong>Simple</strong></td>
<td>Consumer-friendly defaults; Basic mode hides advanced controls and unactivated providers; first-run setup wizard with experience mode selection; one-click transcription</td>
</tr>
<tr>
<td><strong>Background-aware</strong></td>
<td>System tray integration; balloon notifications; minimize-to-tray for long batches</td>
</tr>
<tr>
<td><strong>Cross-platform</strong></td>
<td>Windows 10+ and macOS 12+; CUDA and Apple Silicon Metal GPU detection</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="2-target-users">2. Target Users</h2>
<table>
<thead>
<tr>
<th>Persona</th>
<th>Pain Point</th>
<th>BITS Whisperer Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Journalist</strong></td>
<td>Needs accurate transcripts of interviews</td>
<td>Batch processing, speaker diarization, auto-export</td>
</tr>
<tr>
<td><strong>Student</strong></td>
<td>Lecture recordings on a budget</td>
<td>Free local Whisper models, simple mode</td>
</tr>
<tr>
<td><strong>Researcher</strong></td>
<td>Large datasets of recordings</td>
<td>Folder import, concurrent workers, background mode</td>
</tr>
<tr>
<td><strong>Content Creator</strong></td>
<td>Episode transcripts for SEO &amp; subtitles</td>
<td>SRT/VTT export, multiple providers for quality comparison</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="3-whisper-models-local-inference">3. Whisper Models (Local Inference)</h2>
<p>14 model variants via <strong>faster-whisper</strong> on CPU or CUDA GPU:</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Params</th>
<th>Disk</th>
<th>Min RAM</th>
<th>Min VRAM</th>
<th>Speed</th>
<th>Accuracy</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tiny</td>
<td>39 M</td>
<td>75 MB</td>
<td>2 GB</td>
<td>CPU-OK</td>
<td>5 of 5</td>
<td>2 of 5</td>
<td>99</td>
</tr>
<tr>
<td>Tiny (English)</td>
<td>39 M</td>
<td>75 MB</td>
<td>2 GB</td>
<td>CPU-OK</td>
<td>5 of 5</td>
<td>2 of 5</td>
<td>English only</td>
</tr>
<tr>
<td>Base</td>
<td>74 M</td>
<td>142 MB</td>
<td>2 GB</td>
<td>CPU-OK</td>
<td>4 of 5</td>
<td>3 of 5</td>
<td>99</td>
</tr>
<tr>
<td>Base (English)</td>
<td>74 M</td>
<td>142 MB</td>
<td>2 GB</td>
<td>CPU-OK</td>
<td>4 of 5</td>
<td>3 of 5</td>
<td>English only</td>
</tr>
<tr>
<td>Small</td>
<td>244 M</td>
<td>466 MB</td>
<td>4 GB</td>
<td>2 GB</td>
<td>3 of 5</td>
<td>4 of 5</td>
<td>99</td>
</tr>
<tr>
<td>Small (English)</td>
<td>244 M</td>
<td>466 MB</td>
<td>4 GB</td>
<td>2 GB</td>
<td>3 of 5</td>
<td>4 of 5</td>
<td>English only</td>
</tr>
<tr>
<td>Medium</td>
<td>769 M</td>
<td>1.5 GB</td>
<td>8 GB</td>
<td>4 GB</td>
<td>2 of 5</td>
<td>4 of 5</td>
<td>99</td>
</tr>
<tr>
<td>Medium (English)</td>
<td>769 M</td>
<td>1.5 GB</td>
<td>8 GB</td>
<td>4 GB</td>
<td>2 of 5</td>
<td>5 of 5</td>
<td>English only</td>
</tr>
<tr>
<td>Large v1</td>
<td>1.55 B</td>
<td>3 GB</td>
<td>12 GB</td>
<td>6 GB</td>
<td>1 of 5</td>
<td>5 of 5</td>
<td>99</td>
</tr>
<tr>
<td>Large v2</td>
<td>1.55 B</td>
<td>3 GB</td>
<td>12 GB</td>
<td>6 GB</td>
<td>1 of 5</td>
<td>5 of 5</td>
<td>99</td>
</tr>
<tr>
<td>Large v3</td>
<td>1.55 B</td>
<td>3 GB</td>
<td>12 GB</td>
<td>6 GB</td>
<td>1 of 5</td>
<td>5 of 5</td>
<td>99</td>
</tr>
<tr>
<td>Large v3 Turbo</td>
<td>809 M</td>
<td>1.6 GB</td>
<td>8 GB</td>
<td>4 GB</td>
<td>3 of 5</td>
<td>5 of 5</td>
<td>99</td>
</tr>
<tr>
<td>Distil Large v2 (EN)</td>
<td>756 M</td>
<td>1.5 GB</td>
<td>8 GB</td>
<td>4 GB</td>
<td>4 of 5</td>
<td>4 of 5</td>
<td>English only</td>
</tr>
<tr>
<td>Distil Large v3 (EN)</td>
<td>756 M</td>
<td>1.5 GB</td>
<td>8 GB</td>
<td>4 GB</td>
<td>4 of 5</td>
<td>4 of 5</td>
<td>English only</td>
</tr>
</tbody>
</table>
<p>The <strong>Model Manager</strong> (Ctrl+M) downloads models from HuggingFace and shows
hardware eligibility -- models are tagged as eligible, cautioned, or ineligible
based on the user's CPU, RAM, and GPU detected via <code>DeviceProbe</code>.</p>
<hr>
<h2 id="4-transcription-providers">4. Transcription Providers</h2>
<p>17 adapters implementing <code>TranscriptionProvider</code> ABC:</p>
<table>
<thead>
<tr>
<th>#</th>
<th>Provider</th>
<th>Module</th>
<th>Type</th>
<th>Rate/min</th>
<th>Key Required</th>
<th>Highlights</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Local Whisper</td>
<td><code>local_whisper.py</code></td>
<td>Local</td>
<td>Free</td>
<td>No</td>
<td>Offline, private, GPU-accelerated</td>
</tr>
<tr>
<td>2</td>
<td>Windows Speech</td>
<td><code>windows_speech.py</code></td>
<td>Local</td>
<td>Free</td>
<td>No</td>
<td>SAPI5 + WinRT, offline (Windows only)</td>
</tr>
<tr>
<td>3</td>
<td>Azure Embedded Speech</td>
<td><code>azure_embedded.py</code></td>
<td>Local</td>
<td>Free</td>
<td>No</td>
<td>Microsoft neural models, offline</td>
</tr>
<tr>
<td>4</td>
<td>OpenAI Whisper</td>
<td><code>openai_whisper.py</code></td>
<td>Cloud</td>
<td>$0.006</td>
<td>Yes</td>
<td>Fast, reliable, verbose timestamps</td>
</tr>
<tr>
<td>5</td>
<td>ElevenLabs Scribe</td>
<td><code>elevenlabs_provider.py</code></td>
<td>Cloud</td>
<td>$0.005</td>
<td>Yes</td>
<td>99+ languages, best-in-class accuracy</td>
</tr>
<tr>
<td>6</td>
<td>Groq Whisper</td>
<td><code>groq_whisper.py</code></td>
<td>Cloud</td>
<td>$0.003</td>
<td>Yes</td>
<td>188x real-time on LPU hardware</td>
</tr>
<tr>
<td>7</td>
<td>AssemblyAI</td>
<td><code>assemblyai_provider.py</code></td>
<td>Cloud</td>
<td>$0.011</td>
<td>Yes</td>
<td>Speaker labels, auto-chapters</td>
</tr>
<tr>
<td>8</td>
<td>Deepgram Nova-2</td>
<td><code>deepgram_provider.py</code></td>
<td>Cloud</td>
<td>$0.013</td>
<td>Yes</td>
<td>Smart formatting, fast streaming</td>
</tr>
<tr>
<td>9</td>
<td>Azure Speech Services</td>
<td><code>azure_speech.py</code></td>
<td>Cloud</td>
<td>$0.017</td>
<td>Yes</td>
<td>100+ languages, continuous recognition</td>
</tr>
<tr>
<td>10</td>
<td>Google Speech-to-Text</td>
<td><code>google_speech.py</code></td>
<td>Cloud</td>
<td>$0.024</td>
<td>Yes</td>
<td>Diarization, enhanced models</td>
</tr>
<tr>
<td>11</td>
<td>Google Gemini</td>
<td><code>gemini_provider.py</code></td>
<td>Cloud</td>
<td>$0.0002</td>
<td>Yes</td>
<td>Cheapest cloud, multimodal AI</td>
</tr>
<tr>
<td>12</td>
<td>Amazon Transcribe</td>
<td><code>aws_transcribe.py</code></td>
<td>Cloud</td>
<td>$0.024</td>
<td>Yes</td>
<td>S3 integration, medical vocabularies</td>
</tr>
<tr>
<td>13</td>
<td>Rev.ai</td>
<td><code>rev_ai_provider.py</code></td>
<td>Cloud</td>
<td>$0.020</td>
<td>Yes</td>
<td>Human-hybrid option, high accuracy</td>
</tr>
<tr>
<td>14</td>
<td>Speechmatics</td>
<td><code>speechmatics_provider.py</code></td>
<td>Cloud</td>
<td>$0.017</td>
<td>Yes</td>
<td>50+ languages, real-time streaming</td>
</tr>
<tr>
<td>15</td>
<td>Vosk</td>
<td><code>vosk_provider.py</code></td>
<td>Local</td>
<td>Free</td>
<td>No</td>
<td>Lightweight offline ASR (Kaldi). 20+ languages, 40-50 MB models. Works on very low-end hardware.</td>
</tr>
<tr>
<td>16</td>
<td>Parakeet</td>
<td><code>parakeet_provider.py</code></td>
<td>Local</td>
<td>Free</td>
<td>No</td>
<td>NVIDIA NeMo high-accuracy English ASR. 600M–1.1B param models.</td>
</tr>
<tr>
<td>17</td>
<td>Auphonic</td>
<td><code>auphonic_provider.py</code></td>
<td>Cloud</td>
<td>~$0.01</td>
<td>Yes</td>
<td>Audio post-production + Whisper transcription</td>
</tr>
</tbody>
</table>
<h3 id="provider-selection">Provider Selection</h3>
<p>The <code>ProviderManager</code> maintains a registry of all adapters. The user selects a
provider in Settings, Provider tab. API keys are stored and retrieved via
<code>KeyStore</code> (backed by <code>keyring</code> / Windows Credential Manager). The
<code>TranscriptionService</code> resolves keys automatically at runtime — including
composite keys for AWS (access key + secret key + region).</p>
<h3 id="cloud-provider-onboarding">Cloud Provider Onboarding</h3>
<p>Cloud providers must be <strong>activated</strong> before they appear in Basic mode. The
<code>AddProviderDialog</code> (Tools, then Add Provider) guides the user through a
three-step workflow:</p>
<ol>
<li><strong>Select</strong> a cloud provider from the 12 available options</li>
<li><strong>Enter</strong> the required API key (and any auxiliary credentials like AWS
   region)</li>
<li><strong>Validate</strong> the key with a live test API call</li>
</ol>
<p>On successful validation, the provider's key is stored in <code>KeyStore</code>, and its
identifier is added to <code>GeneralSettings.activated_providers</code>. In Basic mode,
only local providers and activated cloud providers appear in the provider
dropdown. In Advanced mode, all providers are visible regardless of activation.</p>
<p>Each cloud provider's <code>validate_api_key()</code> method makes a real API call:</p>
<table>
<thead>
<tr>
<th>Provider</th>
<th>Validation Method</th>
</tr>
</thead>
<tbody>
<tr>
<td>OpenAI</td>
<td><code>client.models.list()</code></td>
</tr>
<tr>
<td>Google Speech</td>
<td><code>ListOperations</code> via service account credentials</td>
</tr>
<tr>
<td>Azure Speech</td>
<td>Silent WAV <code>recognize_once()</code></td>
</tr>
<tr>
<td>Groq</td>
<td><code>client.models.list()</code></td>
</tr>
<tr>
<td>Deepgram</td>
<td><code>GET /v1/projects</code></td>
</tr>
<tr>
<td>AssemblyAI</td>
<td><code>GET /v2/transcript?limit=1</code></td>
</tr>
<tr>
<td>AWS Transcribe</td>
<td><code>list_transcription_jobs(MaxResults=1)</code></td>
</tr>
<tr>
<td>Gemini</td>
<td><code>genai.list_models()</code></td>
</tr>
<tr>
<td>Rev.ai</td>
<td><code>client.get_account()</code></td>
</tr>
<tr>
<td>Speechmatics</td>
<td><code>GET /v2/jobs?limit=1</code></td>
</tr>
<tr>
<td>ElevenLabs</td>
<td><code>GET /v1/models</code></td>
</tr>
<tr>
<td>Auphonic</td>
<td><code>GET /api/user.json</code></td>
</tr>
</tbody>
</table>
<h3 id="auphonic-integration">Auphonic Integration</h3>
<p>Auphonic provides professional cloud-based audio post-production with built-in
speech recognition. BITS Whisperer integrates Auphonic both as:</p>
<ol>
<li><strong>Transcription Provider</strong> (<code>AuphonicProvider</code>): Creates an Auphonic
   production, applies audio algorithms, runs Whisper speech recognition, and
   returns the transcript.</li>
<li><strong>Standalone Audio Service</strong> (<code>AuphonicService</code>): Processes audio through
   Auphonic's algorithms without transcription — useful as a cloud-based
   preprocessing step.</li>
</ol>
<h4 id="auphonic-api-capabilities">Auphonic API Capabilities</h4>
<table>
<thead>
<tr>
<th>Capability</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Adaptive Leveler</strong></td>
<td>Corrects level differences between speakers, music, and speech</td>
</tr>
<tr>
<td><strong>Loudness Normalization</strong></td>
<td>Target LUFS (-16 podcast, -23 broadcast, -24 TV US)</td>
</tr>
<tr>
<td><strong>Noise &amp; Hum Reduction</strong></td>
<td>Automatic detection; configurable amount (3-100 dB)</td>
</tr>
<tr>
<td><strong>Filtering</strong></td>
<td>High-pass, auto-EQ, bandwidth extension</td>
</tr>
<tr>
<td><strong>Silence &amp; Filler Cutting</strong></td>
<td>Remove silences, filler words, coughs, music segments</td>
</tr>
<tr>
<td><strong>Intro/Outro</strong></td>
<td>Automatically prepend/append audio/video segments</td>
</tr>
<tr>
<td><strong>Chapter Marks</strong></td>
<td>Import/export chapter marks for enhanced podcasts</td>
</tr>
<tr>
<td><strong>Audio Inserts</strong></td>
<td>Insert audio segments at specific offsets (dynamic ad insertion)</td>
</tr>
<tr>
<td><strong>Speech Recognition</strong></td>
<td>Built-in Whisper or external (Google, Amazon, Speechmatics)</td>
</tr>
<tr>
<td><strong>Automatic Shownotes</strong></td>
<td>AI-generated summaries, tags, and chapters (paid feature)</td>
</tr>
<tr>
<td><strong>Multitrack</strong></td>
<td>Process multi-speaker recordings with per-track settings</td>
</tr>
<tr>
<td><strong>Output Formats</strong></td>
<td>MP3, AAC, FLAC, WAV, Opus, Vorbis, ALAC, video</td>
</tr>
<tr>
<td><strong>Publishing</strong></td>
<td>Export to Dropbox, SoundCloud, YouTube, FTP, SFTP, S3, etc.</td>
</tr>
<tr>
<td><strong>Presets</strong></td>
<td>Save and reuse processing configurations</td>
</tr>
<tr>
<td><strong>Webhooks</strong></td>
<td>HTTP POST callbacks when processing completes</td>
</tr>
<tr>
<td><strong>Cuts</strong></td>
<td>Manual cut regions with start/end times</td>
</tr>
<tr>
<td><strong>Fade In/Out</strong></td>
<td>Configurable fade time (0–5000 ms)</td>
</tr>
</tbody>
</table>
<h4 id="auphonic-authentication">Auphonic Authentication</h4>
<table>
<thead>
<tr>
<th>Method</th>
<th>Use Case</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>API Key</strong></td>
<td>Personal scripts, BITS Whisperer</td>
<td>Bearer token from Account Settings page</td>
</tr>
<tr>
<td><strong>HTTP Basic Auth</strong></td>
<td>Simple scripts</td>
<td>Username + password (not recommended for apps)</td>
</tr>
<tr>
<td><strong>OAuth 2.0 (Web)</strong></td>
<td>Third-party web applications</td>
<td>Client ID/Secret + redirect URI + grant code</td>
</tr>
<tr>
<td><strong>OAuth 2.0 (Desktop)</strong></td>
<td>Desktop/mobile apps</td>
<td>Client ID/Secret + username/password exchange</td>
</tr>
</tbody>
</table>
<p>BITS Whisperer uses <strong>API Key authentication</strong>. The user generates a token at
<a href="https://auphonic.com/accounts/settings/#api-key">https://auphonic.com/accounts/settings/#api-key</a> and stores it via the
Providers &amp; Keys settings tab. The key is persisted in <code>KeyStore</code> (Windows
Credential Manager) as <code>"auphonic"</code> to <code>"Auphonic API Token"</code>.</p>
<h4 id="auphonic-pricing">Auphonic Pricing</h4>
<table>
<thead>
<tr>
<th>Plan</th>
<th>Recurring Credits</th>
<th>One-Time Credits</th>
<th>Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td>Free</td>
<td>2 hours/month</td>
<td>None</td>
<td>$0</td>
</tr>
<tr>
<td>Starter</td>
<td>9 hours/month</td>
<td>None</td>
<td>$11/month</td>
</tr>
<tr>
<td>Professional</td>
<td>45 hours/month</td>
<td>None</td>
<td>$49/month</td>
</tr>
<tr>
<td>Enterprise</td>
<td>Custom</td>
<td>Custom</td>
<td>Contact</td>
</tr>
<tr>
<td>Pay-as-you-go</td>
<td>None</td>
<td>Purchased blocks</td>
<td>~$0.01/min</td>
</tr>
</tbody>
</table>
<h4 id="auphonic-api-endpoints">Auphonic API Endpoints</h4>
<table>
<thead>
<tr>
<th>Endpoint</th>
<th>Method</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>/api/user.json</code></td>
<td>GET</td>
<td>Account info &amp; credits</td>
</tr>
<tr>
<td><code>/api/productions.json</code></td>
<td>POST</td>
<td>Create production</td>
</tr>
<tr>
<td><code>/api/production/{uuid}.json</code></td>
<td>GET</td>
<td>Get/update production details</td>
</tr>
<tr>
<td><code>/api/production/{uuid}/upload.json</code></td>
<td>POST</td>
<td>Upload audio/image files</td>
</tr>
<tr>
<td><code>/api/production/{uuid}/start.json</code></td>
<td>POST</td>
<td>Start audio processing</td>
</tr>
<tr>
<td><code>/api/production/{uuid}/status.json</code></td>
<td>GET</td>
<td>Poll production status</td>
</tr>
<tr>
<td><code>/api/production/{uuid}/publish.json</code></td>
<td>POST</td>
<td>Publish to outgoing services</td>
</tr>
<tr>
<td><code>/api/simple/productions.json</code></td>
<td>POST</td>
<td>Simple API (one-shot upload+process)</td>
</tr>
<tr>
<td><code>/api/presets.json</code></td>
<td>GET/POST</td>
<td>List/create presets</td>
</tr>
<tr>
<td><code>/api/preset/{uuid}.json</code></td>
<td>GET</td>
<td>Get preset details</td>
</tr>
<tr>
<td><code>/api/services.json</code></td>
<td>GET</td>
<td>List external services</td>
</tr>
<tr>
<td><code>/api/info/algorithms.json</code></td>
<td>GET</td>
<td>Available audio algorithms</td>
</tr>
<tr>
<td><code>/api/info/output_files.json</code></td>
<td>GET</td>
<td>Available output formats</td>
</tr>
<tr>
<td><code>/api/info/production_status.json</code></td>
<td>GET</td>
<td>Status code reference</td>
</tr>
<tr>
<td><code>/api/download/audio-result/{uuid}/{file}</code></td>
<td>GET</td>
<td>Download processed files</td>
</tr>
</tbody>
</table>
<h3 id="provider-specific-settings">Provider-Specific Settings</h3>
<p>Each cloud provider exposes its unique configurable options during onboarding
via the Add Provider dialog. Settings are stored in <code>ProviderDefaultSettings</code>
and applied automatically via the <code>configure()</code> method before each
transcription.</p>
<table>
<thead>
<tr>
<th>Provider</th>
<th>Configurable Settings</th>
</tr>
</thead>
<tbody>
<tr>
<td>Auphonic</td>
<td>Leveler, loudness target, noise/hum reduction, silence/filler/cough cutting, speech engine, output format, crosstalk</td>
</tr>
<tr>
<td>Deepgram</td>
<td>Model (nova-2/nova/enhanced/base), smart format, punctuation, paragraphs, utterances</td>
</tr>
<tr>
<td>AssemblyAI</td>
<td>Punctuation, formatting, auto chapters, content safety, sentiment analysis, entity detection</td>
</tr>
<tr>
<td>Google Speech</td>
<td>Recognition model, max speaker count</td>
</tr>
<tr>
<td>Azure</td>
<td>Custom endpoint ID</td>
</tr>
<tr>
<td>AWS</td>
<td>Max speaker labels</td>
</tr>
<tr>
<td>Speechmatics</td>
<td>Operating point (enhanced/standard)</td>
</tr>
<tr>
<td>ElevenLabs</td>
<td>Timestamp granularity (segment/word)</td>
</tr>
<tr>
<td>OpenAI</td>
<td>Model, temperature</td>
</tr>
<tr>
<td>Groq</td>
<td>Model (v3-turbo/v3/distil)</td>
</tr>
<tr>
<td>Gemini</td>
<td>Model (2.0-flash/1.5-flash/1.5-pro)</td>
</tr>
<tr>
<td>Rev.ai</td>
<td>Skip diarization</td>
</tr>
</tbody>
</table>
<h3 id="ai-services-translation-summarization-chat">AI Services (Translation, Summarization &amp; Chat)</h3>
<p>The <code>AIService</code> class (in <code>core/ai_service.py</code>) provides translation and
summarization of transcripts via <strong>6 pluggable AI providers</strong>:</p>
<table>
<thead>
<tr>
<th>#</th>
<th>Provider</th>
<th>Module/Class</th>
<th>Models</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>OpenAI</td>
<td><code>OpenAIAIProvider</code></td>
<td>gpt-4o, gpt-4o-mini</td>
<td>Fastest, most reliable</td>
</tr>
<tr>
<td>2</td>
<td>Anthropic</td>
<td><code>AnthropicAIProvider</code></td>
<td>Claude Sonnet 4, Claude Haiku</td>
<td>Strong for long transcripts</td>
</tr>
<tr>
<td>3</td>
<td>Azure OpenAI</td>
<td><code>AzureOpenAIProvider</code></td>
<td>Configurable deployment</td>
<td>Enterprise-grade, GDPR compliant</td>
</tr>
<tr>
<td>4</td>
<td>Google Gemini</td>
<td><code>GeminiAIProvider</code></td>
<td>Gemini 2.0 Flash, 1.5 Flash/Pro</td>
<td>Fast, affordable</td>
</tr>
<tr>
<td>5</td>
<td>GitHub Copilot</td>
<td><code>CopilotAIProvider</code></td>
<td>gpt-4o (via Copilot SDK)</td>
<td>Interactive chat &amp; tool-augmented</td>
</tr>
<tr>
<td>6</td>
<td>Ollama</td>
<td><code>OllamaAIProvider</code></td>
<td>Any Ollama/GGUF model (Llama, Mistral, Gemma, Phi, etc.)</td>
<td>Local, free, private — no API key required</td>
</tr>
</tbody>
</table>
<p>AI features are accessed via the <strong>AI</strong> menu:</p>
<ul>
<li><strong>Translate</strong> (Ctrl+T): Translates the transcript to the configured target
  language (or multiple languages simultaneously)</li>
<li><strong>Summarize</strong> (Ctrl+Shift+S): Generates concise, detailed, or bullet-point
  summaries</li>
<li><strong>Copilot Chat</strong> (Ctrl+Shift+C): Opens the interactive chat panel for Q&amp;A</li>
<li><strong>AI Action Builder</strong>: Opens the template editor for creating
  post-transcription AI processing templates</li>
<li><strong>AI Provider Settings</strong>: Configures AI providers (6 providers including
  Ollama)</li>
</ul>
<p>AI provider settings (<code>AISettings</code> dataclass) include:</p>
<ul>
<li><code>selected_provider</code> (openai/anthropic/azure_openai/gemini/copilot/ollama)</li>
<li><code>openai_model</code>, <code>anthropic_model</code>, <code>gemini_model</code>, <code>copilot_model</code>,
  <code>ollama_model</code></li>
<li><code>ollama_endpoint</code>, <code>ollama_custom_model</code></li>
<li><code>temperature</code>, <code>max_tokens</code></li>
<li><code>translation_language</code>, <code>summarization_style</code></li>
<li><code>multi_target_languages</code> — list of languages for simultaneous translation</li>
<li><code>custom_vocabulary</code> — domain-specific terms for improved AI accuracy</li>
<li><code>active_translation_template</code>, <code>active_summarization_template</code> — selected
  prompt template IDs</li>
<li><code>custom_prompt_templates</code> — user-defined prompt templates</li>
<li><code>context_strategy</code> — transcript fitting strategy
  (smart/truncate/tail/head_tail)</li>
<li><code>context_transcript_budget_pct</code> — fraction of context window for transcript
  (default 0.70)</li>
<li><code>context_response_reserve_tokens</code> — tokens reserved for AI response (default
  4096)</li>
<li><code>context_max_conversation_turns</code> — max conversation turns to keep (default 20)</li>
</ul>
<h3 id="ai-model-catalog">AI Model Catalog</h3>
<p>The <code>constants.py</code> module defines a comprehensive AI model catalog with
real-time pricing information via the <code>AIModelInfo</code> frozen dataclass:</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>id</code></td>
<td>str</td>
<td>Model identifier</td>
</tr>
<tr>
<td><code>name</code></td>
<td>str</td>
<td>Display name</td>
</tr>
<tr>
<td><code>provider</code></td>
<td>str</td>
<td>Provider key (openai/anthropic/gemini/copilot)</td>
</tr>
<tr>
<td><code>description</code></td>
<td>str</td>
<td>Human-readable description</td>
</tr>
<tr>
<td><code>input_price_per_1m</code></td>
<td>float</td>
<td>USD per 1M input tokens (0 = included)</td>
</tr>
<tr>
<td><code>output_price_per_1m</code></td>
<td>float</td>
<td>USD per 1M output tokens (0 = included)</td>
</tr>
<tr>
<td><code>context_window</code></td>
<td>int</td>
<td>Maximum context tokens</td>
</tr>
<tr>
<td><code>max_output_tokens</code></td>
<td>int</td>
<td>Maximum output tokens</td>
</tr>
<tr>
<td><code>copilot_tier</code></td>
<td>str</td>
<td>Required Copilot tier (empty if not Copilot)</td>
</tr>
<tr>
<td><code>is_premium</code></td>
<td>bool</td>
<td>Requires premium Copilot subscription</td>
</tr>
<tr>
<td><code>supports_streaming</code></td>
<td>bool</td>
<td>Whether the model supports streaming</td>
</tr>
</tbody>
</table>
<h4 id="model-counts-by-provider">Model Counts by Provider</h4>
<table>
<thead>
<tr>
<th>Provider</th>
<th>Count</th>
<th>Models</th>
</tr>
</thead>
<tbody>
<tr>
<td>OpenAI</td>
<td>4</td>
<td>GPT-4o Mini, GPT-4o, GPT-4 Turbo, GPT-3.5 Turbo</td>
</tr>
<tr>
<td>Anthropic</td>
<td>3</td>
<td>Claude Sonnet 4, Claude Haiku 4, Claude 3.5 Sonnet</td>
</tr>
<tr>
<td>Gemini</td>
<td>8</td>
<td>Gemini 2.0 Flash, 2.5 Pro, 2.5 Flash + Gemma 27B/12B/4B/1B/3n-E4B</td>
</tr>
<tr>
<td>Copilot</td>
<td>7</td>
<td>GPT-4o Mini, GPT-4o, GPT-4 Turbo, Claude Sonnet 4, Claude Haiku 4, o3-mini, Gemini 2.0 Flash</td>
</tr>
<tr>
<td><strong>Total</strong></td>
<td><strong>22</strong></td>
<td>All models across all providers</td>
</tr>
</tbody>
</table>
<p>Helper functions: <code>get_ai_model_by_id()</code>, <code>get_models_for_provider()</code>,
<code>get_copilot_models_for_tier()</code>, <code>format_price_per_1k()</code>.</p>
<h3 id="copilot-subscription-tiers">Copilot Subscription Tiers</h3>
<p>Copilot models are gated by subscription tier via <code>COPILOT_TIERS</code>:</p>
<table>
<thead>
<tr>
<th>Tier</th>
<th>Price</th>
<th>Available Models</th>
</tr>
</thead>
<tbody>
<tr>
<td>Free</td>
<td>$0</td>
<td>GPT-4o Mini</td>
</tr>
<tr>
<td>Pro</td>
<td>$10/month</td>
<td>All 7 models including premium (Claude, o3-mini, Gemini)</td>
</tr>
<tr>
<td>Business</td>
<td>$19/user/month</td>
<td>All Pro models + organization admin controls</td>
</tr>
<tr>
<td>Enterprise</td>
<td>$39/user/month</td>
<td>All models + knowledge bases, fine-tuning, compliance</td>
</tr>
</tbody>
</table>
<p>The <code>CopilotSettings.subscription_tier</code> field (default: "pro") controls which
models appear in the model selector. <code>get_copilot_models_for_tier()</code> returns
only models at or below the user's tier level.</p>
<h3 id="prompt-templates">Prompt Templates</h3>
<p>10 built-in prompt templates (<code>BUILTIN_PROMPT_TEMPLATES</code>) via the
<code>PromptTemplate</code> frozen dataclass:</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>id</code></td>
<td>str</td>
<td>Template identifier</td>
</tr>
<tr>
<td><code>name</code></td>
<td>str</td>
<td>Display name</td>
</tr>
<tr>
<td><code>category</code></td>
<td>str</td>
<td>translation / summarization / analysis</td>
</tr>
<tr>
<td><code>description</code></td>
<td>str</td>
<td>Human-readable description</td>
</tr>
<tr>
<td><code>template</code></td>
<td>str</td>
<td>Prompt text with <code>{text}</code> and <code>{language}</code> placeholders</td>
</tr>
<tr>
<td><code>is_builtin</code></td>
<td>bool</td>
<td>True for built-in, False for user-created</td>
</tr>
</tbody>
</table>
<h4 id="template-breakdown">Template Breakdown</h4>
<table>
<thead>
<tr>
<th>Category</th>
<th>Count</th>
<th>Templates</th>
</tr>
</thead>
<tbody>
<tr>
<td>Translation</td>
<td>4</td>
<td>Standard, Informal, Technical, Legal</td>
</tr>
<tr>
<td>Summarization</td>
<td>4</td>
<td>Concise Summary, Detailed Summary, Bullet Points, Meeting Minutes</td>
</tr>
<tr>
<td>Analysis</td>
<td>2</td>
<td>Sentiment Analysis, Extract Questions</td>
</tr>
</tbody>
</table>
<p>Active templates are tracked in <code>AISettings.active_translation_template</code> and
<code>AISettings.active_summarization_template</code>. Users can also create custom
templates stored in <code>AISettings.custom_prompt_templates</code>.</p>
<p>Helper functions: <code>get_prompt_template_by_id()</code>, <code>get_templates_by_category()</code>.</p>
<h3 id="custom-vocabulary">Custom Vocabulary</h3>
<p><code>AISettings.custom_vocabulary</code> stores a list of domain-specific terms (acronyms,
proper nouns, technical jargon) that are injected into AI prompts to improve
translation and summarization accuracy. The vocabulary is appended to the prompt
context before sending to the AI provider.</p>
<h3 id="multi-language-simultaneous-translation">Multi-Language Simultaneous Translation</h3>
<p><code>AISettings.multi_target_languages</code> stores a list of target language codes.
<code>AIService.translate_multi()</code> iterates over each language and calls
<code>translate()</code> independently, returning a dict mapping each language to its
<code>AIResponse</code>. This enables one-click translation to multiple languages.</p>
<h3 id="real-time-streaming-transcription">Real-Time Streaming Transcription</h3>
<p>The <code>ProviderCapabilities.supports_streaming</code> field indicates whether a
transcription provider supports real-time streaming:</p>
<table>
<thead>
<tr>
<th>Provider</th>
<th class="text-center">Streaming</th>
</tr>
</thead>
<tbody>
<tr>
<td>Deepgram</td>
<td class="text-center">Yes</td>
</tr>
<tr>
<td>AssemblyAI</td>
<td class="text-center">Yes</td>
</tr>
<tr>
<td>All others</td>
<td class="text-center">No</td>
</tr>
</tbody>
</table>
<h3 id="github-copilot-sdk-integration">GitHub Copilot SDK Integration</h3>
<p>The <code>CopilotService</code> class (in <code>core/copilot_service.py</code>) integrates the GitHub
Copilot SDK for interactive AI-powered transcript analysis:</p>
<h4 id="copilotservice">CopilotService</h4>
<ul>
<li><strong>Async SDK client</strong> with process management for the Copilot CLI</li>
<li><strong>Session management</strong> — conversation history maintained per session</li>
<li><strong>Streaming responses</strong> — real-time token-by-token response delivery</li>
<li><strong>Custom tools</strong> — transcript-aware tools that let the agent access and
  analyze the current transcript</li>
<li><strong>Agent configuration</strong> — name, instructions, persona, and welcome message</li>
<li><strong>CLI detection</strong> — auto-detects <code>github-copilot-cli</code> on PATH or manual path</li>
</ul>
<h4 id="interactive-ai-chat-panel-uicopilot_chat_panelpy">Interactive AI Chat Panel (<code>ui/copilot_chat_panel.py</code>)</h4>
<ul>
<li><strong>Toggle</strong>: Ctrl+Shift+C or AI, then Copilot Chat</li>
<li><strong>Streaming display</strong> — responses appear token-by-token</li>
<li><strong>Quick actions</strong> — one-click buttons for common tasks (summarize, key points,
  speakers, action items)</li>
<li><strong>Transcript context</strong> — automatically provides the current transcript to the
  agent</li>
<li><strong>New conversation</strong> — clear history and start fresh</li>
<li><strong>Slash commands</strong> — 28 built-in <code>/commands</code> for AI analysis (summarize,
  translate, key-points, action-items, topics, speakers, search, ask, run, copy)
  and app actions (help, clear, status, provider, export, open, start, pause,
  cancel, clear-queue, retry, settings, live, models, agent, history,
  open-folder, context)</li>
<li><strong>Autocomplete</strong> — <code>wx.PopupTransientWindow</code> appears as the user types <code>/</code>,
  with keyboard navigation (Up/Down/Tab/Enter/Escape)</li>
</ul>
<h4 id="slash-command-system-uislash_commandspy">Slash Command System (<code>ui/slash_commands.py</code>)</h4>
<p>Extensible registry of chat commands with:</p>
<ul>
<li><strong><code>SlashCommand</code> dataclass</strong> — name, description, category, handler, aliases,
  arg_hint, requires_transcript</li>
<li><strong><code>SlashCommandRegistry</code></strong> — register/get/match (prefix + substring
  autocomplete), categories, alias resolution</li>
<li><strong><code>parse_slash_command()</code></strong> — regex parser for <code>/command args</code> input</li>
<li><strong><code>build_default_registry()</code></strong> — factory returning 28 built-in commands in two
  categories (AI, App)</li>
</ul>
<h4 id="copilot-setup-wizard-uicopilot_setup_dialogpy">Copilot Setup Wizard (<code>ui/copilot_setup_dialog.py</code>)</h4>
<p>Four-step guided setup dialog:</p>
<ol>
<li><strong>CLI Install</strong> — Checks for GitHub Copilot CLI; offers WinGet install on
   Windows</li>
<li><strong>SDK Install</strong> — Installs the Copilot SDK Python package</li>
<li><strong>Authentication</strong> — Authenticates with GitHub via CLI device flow</li>
<li><strong>Test</strong> — Runs a connection test to verify everything works</li>
</ol>
<h4 id="document-reader-coredocument_readerpy">Document Reader (<code>core/document_reader.py</code>)</h4>
<p>Utility module that extracts plain text from multiple file formats for use as AI
context. Supports plain text (.txt, .md, .csv, .log, .json, .xml, .yaml), Word
documents (.docx via python-docx), spreadsheets (.xlsx/.xls via openpyxl), PDF
(.pdf via pypdf), and RTF (.rtf via striprtf). Enforces a 10 MB file size limit
and provides graceful fallbacks when optional libraries are not installed.</p>
<h4 id="ai-action-builder-uiagent_builder_dialogpy">AI Action Builder (<code>ui/agent_builder_dialog.py</code>)</h4>
<p>Five-tab guided dialog for creating reusable post-transcription AI processing
templates. Templates define system prompts, model parameters, processing goals,
and document attachments that are automatically applied after transcription
completes.</p>
<table>
<thead>
<tr>
<th>Tab</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Identity</strong></td>
<td>Action name, description</td>
</tr>
<tr>
<td><strong>Instructions</strong></td>
<td>System prompt with 8 built-in presets</td>
</tr>
<tr>
<td><strong>Tools</strong></td>
<td>Enable/disable transcript-aware tools</td>
</tr>
<tr>
<td><strong>Welcome</strong></td>
<td>Set the greeting message for the chat panel</td>
</tr>
<tr>
<td><strong>Attachments</strong></td>
<td>Attach reference documents (glossaries, style guides, etc.) to provide additional context for AI processing</td>
</tr>
</tbody>
</table>
<p>8 built-in presets:</p>
<table>
<thead>
<tr>
<th>Preset</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Meeting Minutes</td>
<td>Formal minutes with attendees, decisions, action items</td>
</tr>
<tr>
<td>Action Items</td>
<td>Extract to-do items, deadlines, and responsibilities</td>
</tr>
<tr>
<td>Executive Summary</td>
<td>Brief overview for leadership/stakeholders</td>
</tr>
<tr>
<td>Interview Notes</td>
<td>Key points, themes, and notable quotes</td>
</tr>
<tr>
<td>Lecture Notes</td>
<td>Educational content structured for study</td>
</tr>
<tr>
<td>Q&amp;A Extraction</td>
<td>Questions and answers identified and paired</td>
</tr>
<tr>
<td>General Assistant</td>
<td>General-purpose transcript analysis</td>
</tr>
<tr>
<td>Custom</td>
<td>Blank template for custom instructions</td>
</tr>
</tbody>
</table>
<p>Templates are saved as JSON files in <code>DATA_DIR/agents/</code> using the <code>AgentConfig</code>
dataclass from <code>copilot_service.py</code>. The <code>AgentConfig</code> includes an
<code>attachments: list[Attachment]</code> field for document attachments. Each
<code>Attachment</code> has <code>file_path</code>, <code>instructions</code> (per-attachment guidance), and
<code>display_name</code> fields with full serialization support.</p>
<h3 id="ai-actions-post-transcription-processing">AI Actions (Post-Transcription Processing)</h3>
<p>AI Actions allow automatic AI processing of transcripts immediately after
transcription completes. The feature works with any configured AI provider
(OpenAI, Anthropic, Azure OpenAI, Gemini, Copilot, or Ollama).</p>
<h4 id="pipeline-integration">Pipeline Integration</h4>
<p>After a job completes transcription (Step 5 in the pipeline), the service checks
<code>job.ai_action_template</code>. If set, it:</p>
<ol>
<li>Sets <code>job.ai_action_status = "running"</code></li>
<li>Resolves the template instructions (built-in preset or saved file)</li>
<li>Reads document attachments via <code>_build_attachments_text()</code> using
   <code>DocumentReader</code> (extracts text from DOCX, PDF, XLSX, RTF, TXT, and other
   formats; 10 MB size limit)</li>
<li>Fits the transcript to the model's context window using
   <code>ContextWindowManager</code> (smart strategy auto-selects truncate or head+tail
   based on overflow ratio; attachment tokens are accounted for in the budget,
   reducing transcript space proportionally)</li>
<li>Lazy-imports <code>AIService</code> and calls <code>provider.generate()</code></li>
<li>Stores the result in <code>job.ai_action_result</code> or error in <code>job.ai_action_error</code></li>
<li>Updates <code>job.ai_action_status</code> to "completed" or "failed"</li>
</ol>
<h4 id="job-model-fields">Job Model Fields</h4>
<table>
<thead>
<tr>
<th>Field</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ai_action_template</code></td>
<td>str</td>
<td>Built-in preset name or path to AgentConfig JSON</td>
</tr>
<tr>
<td><code>ai_action_result</code></td>
<td>str</td>
<td>Output text from AI processing</td>
</tr>
<tr>
<td><code>ai_action_status</code></td>
<td>str</td>
<td>"", "running", "completed", "failed"</td>
</tr>
<tr>
<td><code>ai_action_error</code></td>
<td>str</td>
<td>Error message if processing failed</td>
</tr>
<tr>
<td><code>ai_action_attachments</code></td>
<td>list[dict]</td>
<td>Per-job attachment overrides (file_path, instructions, display_name)</td>
</tr>
</tbody>
</table>
<h4 id="built-in-presets-6">Built-in Presets (6)</h4>
<table>
<thead>
<tr>
<th>Preset</th>
<th>Instructions Summary</th>
</tr>
</thead>
<tbody>
<tr>
<td>Meeting Minutes</td>
<td>Formal meeting minutes with attendees and action items</td>
</tr>
<tr>
<td>Action Items</td>
<td>Extract tasks, deadlines, and assigned responsibilities</td>
</tr>
<tr>
<td>Executive Summary</td>
<td>Brief overview highlighting key points and decisions</td>
</tr>
<tr>
<td>Interview Notes</td>
<td>Key discussion points, themes, and notable quotes</td>
</tr>
<tr>
<td>Lecture Notes</td>
<td>Structured educational notes for study and review</td>
</tr>
<tr>
<td>Q&amp;A Extraction</td>
<td>Identify and pair all questions with their answers</td>
</tr>
</tbody>
</table>
<h4 id="user-interface">User Interface</h4>
<ul>
<li><strong>AddFileWizard</strong>: AI Action dropdown with "None (transcribe only)" + built-in
  presets + saved templates (prefixed with ★)</li>
<li><strong>Queue Panel</strong>: Status indicators — ⭐ (pending), ⏳ (running), ✓ (completed),
  ✗ (failed)</li>
<li><strong>Transcript Panel</strong>: AI Action result section below the transcript with
  label, text area, and Copy button. Dynamically shown/hidden based on job
  status.</li>
<li><strong>Main Frame</strong>: AI action status announced to screen readers; transcript panel
  auto-refreshes on completion</li>
</ul>
<h3 id="context-window-management">Context Window Management</h3>
<p>The <code>ContextWindowManager</code> class (in <code>core/context_manager.py</code>) provides
model-aware token budgeting for all AI interactions — chat, AI actions, and
slash commands. It replaces the previous hardcoded 50,000-character truncation
with intelligent, per-model context allocation.</p>
<h4 id="token-estimation">Token Estimation</h4>
<table>
<thead>
<tr>
<th>Method</th>
<th>Approach</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>estimate_tokens()</code></td>
<td>Heuristic: <code>len(text) / chars_per_token</code> (default 4.0)</td>
</tr>
<tr>
<td><code>estimate_tokens_precise()</code></td>
<td>Exact: tiktoken encoding (OpenAI models only)</td>
</tr>
<tr>
<td><code>count_tokens()</code></td>
<td>Unified: tries tiktoken first, falls back to heuristic</td>
</tr>
</tbody>
</table>
<h4 id="model-context-window-lookup">Model Context Window Lookup</h4>
<p><code>get_model_context_window(model, provider)</code> resolves context sizes:</p>
<ol>
<li>Exact match in <code>AIModelInfo</code> catalog (22+ models with known context windows)</li>
<li>Heuristic fallback for unknown models (e.g., GPT-4 → 128K, Claude → 200K,
   Gemini → 1M, Ollama → 8K)</li>
<li>Ultimate fallback: 16,000 tokens</li>
</ol>
<h4 id="transcript-fitting-strategies">Transcript Fitting Strategies</h4>
<table>
<thead>
<tr>
<th>Strategy</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>truncate</code></td>
<td>Keep the beginning of the transcript (previous default)</td>
</tr>
<tr>
<td><code>tail</code></td>
<td>Keep the end — useful for recent context</td>
</tr>
<tr>
<td><code>head_tail</code></td>
<td>Keep beginning and end, elide middle with marker</td>
</tr>
<tr>
<td><code>smart</code></td>
<td>Auto-selects: truncate if overflow ≤1.3×, head_tail otherwise</td>
</tr>
</tbody>
</table>
<h4 id="context-budget-allocation">Context Budget Allocation</h4>
<p><code>prepare_chat_context()</code> orchestrates the full budget:</p>
<ol>
<li>Looks up model context window</li>
<li>Subtracts response reserve (default 4,096 tokens)</li>
<li>Estimates system prompt token cost</li>
<li>Allocates transcript budget (default 70% of remaining)</li>
<li>Fits transcript using the configured strategy</li>
<li>Trims conversation history (max turns + remaining token budget)</li>
<li>Returns <code>PreparedContext</code> with budget breakdown</li>
</ol>
<p><code>prepare_action_context()</code> handles one-shot AI actions with the same budget
logic but no conversation history. It accepts an optional <code>attachments_text</code>
parameter; when provided, attachment tokens are subtracted from the available
budget before fitting the transcript.</p>
<h4 id="settings">Settings</h4>
<table>
<thead>
<tr>
<th>Field</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>context_strategy</code></td>
<td>str</td>
<td>"smart"</td>
<td>Fitting strategy</td>
</tr>
<tr>
<td><code>context_transcript_budget_pct</code></td>
<td>float</td>
<td>0.70</td>
<td>Fraction of window for transcript</td>
</tr>
<tr>
<td><code>context_response_reserve_tokens</code></td>
<td>int</td>
<td>4096</td>
<td>Tokens reserved for response</td>
</tr>
<tr>
<td><code>context_max_conversation_turns</code></td>
<td>int</td>
<td>20</td>
<td>Max conversation turns kept</td>
</tr>
</tbody>
</table>
<h4 id="integration-points">Integration Points</h4>
<ul>
<li><code>ai_service.py</code> <code>chat()</code> — uses <code>prepare_chat_context()</code> for multi-turn chat</li>
<li><code>copilot_service.py</code> <code>_create_session()</code> — fits transcript for Copilot
  sessions</li>
<li><code>transcription_service.py</code> <code>_run_ai_action()</code> — fits transcript for
  post-transcription actions; <code>_build_attachments_text()</code> reads document
  attachments via <code>DocumentReader</code></li>
<li><code>slash_commands.py</code> <code>/summarize</code> and <code>/run</code> — fits transcript for slash
  command execution</li>
<li><code>copilot_chat_panel.py</code> — displays context budget in status bar (e.g.,
  "Context: 45K/128K tokens (35%)")</li>
<li><code>/context</code> slash command — shows detailed budget breakdown in chat</li>
</ul>
<h4 id="copilotsettings-dataclass-11-fields">CopilotSettings Dataclass (11 fields)</h4>
<table>
<thead>
<tr>
<th>Field</th>
<th>Type</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>enabled</code></td>
<td>bool</td>
<td>False</td>
</tr>
<tr>
<td><code>cli_path</code></td>
<td>str</td>
<td>"" (auto-detect)</td>
</tr>
<tr>
<td><code>use_logged_in_user</code></td>
<td>bool</td>
<td>True</td>
</tr>
<tr>
<td><code>default_model</code></td>
<td>str</td>
<td>"gpt-4o"</td>
</tr>
<tr>
<td><code>streaming</code></td>
<td>bool</td>
<td>True</td>
</tr>
<tr>
<td><code>system_message</code></td>
<td>str</td>
<td>Transcript assistant msg</td>
</tr>
<tr>
<td><code>agent_name</code></td>
<td>str</td>
<td>"BITS Transcript Assistant"</td>
</tr>
<tr>
<td><code>agent_instructions</code></td>
<td>str</td>
<td>""</td>
</tr>
<tr>
<td><code>auto_start_cli</code></td>
<td>bool</td>
<td>True</td>
</tr>
<tr>
<td><code>allow_transcript_tools</code></td>
<td>bool</td>
<td>True</td>
</tr>
<tr>
<td><code>chat_panel_visible</code></td>
<td>bool</td>
<td>False</td>
</tr>
</tbody>
</table>
<h3 id="speaker-diarization">Speaker Diarization</h3>
<p>Speaker diarization (identifying who spoke when) is supported through two
mechanisms:</p>
<h4 id="cloud-provider-diarization">Cloud Provider Diarization</h4>
<p>10 cloud providers support built-in diarization when "Include speaker labels" is
enabled:</p>
<table>
<thead>
<tr>
<th>Provider</th>
<th>Max Speakers</th>
<th>Implementation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Azure Speech</td>
<td>Configurable</td>
<td><code>ConversationTranscriber</code> with <code>speaker_id</code> extraction</td>
</tr>
<tr>
<td>Google Speech</td>
<td>Configurable</td>
<td><code>diarization_config</code> on recognition request</td>
</tr>
<tr>
<td>Deepgram</td>
<td>Auto</td>
<td>Nova-2 <code>diarize=true</code> parameter</td>
</tr>
<tr>
<td>AssemblyAI</td>
<td>Auto</td>
<td><code>speaker_labels=True</code> feature</td>
</tr>
<tr>
<td>Amazon Transcribe</td>
<td>Configurable</td>
<td><code>ShowSpeakerLabels</code> in settings</td>
</tr>
<tr>
<td>ElevenLabs</td>
<td>Auto</td>
<td>Built-in <code>diarize</code> parameter</td>
</tr>
<tr>
<td>Rev.ai</td>
<td>Auto</td>
<td>Automatic speaker detection</td>
</tr>
<tr>
<td>Speechmatics</td>
<td>Auto</td>
<td>Speaker change detection</td>
</tr>
<tr>
<td>Google Gemini</td>
<td>Auto</td>
<td>Multimodal speaker detection</td>
</tr>
<tr>
<td>Auphonic</td>
<td>n/a</td>
<td>Post-production only (no diarization)</td>
</tr>
</tbody>
</table>
<h4 id="cloud-free-local-diarization-corediarizationpy">Cloud-Free Local Diarization (<code>core/diarization.py</code>)</h4>
<p>Optional privacy-first speaker detection using <strong>pyannote.audio</strong>:</p>
<ul>
<li><strong><code>LocalDiarizer</code></strong> class wraps <code>pyannote.audio.Pipeline</code> with lazy loading</li>
<li><strong><code>diarize(audio_path, min_speakers, max_speakers)</code></strong> returns
  <code>list[SpeakerTurn]</code></li>
<li><strong><code>apply_to_transcript(result, turns)</code></strong> merges diarization with transcription
  segments by temporal overlap</li>
<li><strong><code>apply_speaker_map(result, speaker_map)</code></strong> renames speakers
  post-transcription</li>
<li>Requires HuggingFace auth token for gated models (stored in <code>KeyStore</code>)</li>
<li>Works as post-processing on ANY provider's output</li>
</ul>
<p>Configuration via <code>DiarizationSettings</code>:</p>
<ul>
<li><code>enabled</code> (default True), <code>max_speakers</code> (10), <code>min_speakers</code> (2)</li>
<li><code>use_local_diarization</code> (False), <code>local_engine</code> ("pyannote")</li>
<li><code>pyannote_model</code> ("pyannote/speaker-diarization-3.1")</li>
<li><code>speaker_map</code> (dict mapping internal IDs to display names)</li>
</ul>
<h4 id="speaker-editing-post-transcription">Speaker Editing (Post-Transcription)</h4>
<p>The transcript panel provides speaker management after transcription:</p>
<ul>
<li><strong>Manage Speakers</strong> button opens <code>SpeakerRenameDialog</code> with editable name
  fields for all detected speakers (e.g., rename "Speaker 1" to "Alice")</li>
<li><strong>Right-click context menu</strong> on any transcript line offers "Assign to Speaker"
  submenu and "New Speaker..." option</li>
<li><strong>Display format</strong>: <code>[timestamp]  SpeakerName: text</code> for natural reading</li>
<li><strong>Instant global updates</strong> -- all renames applied immediately to the full
  transcript via the <code>speaker_map</code> on <code>TranscriptionResult</code></li>
</ul>
<hr>
<h2 id="5-audio-preprocessing-pipeline">5. Audio Preprocessing Pipeline</h2>
<p>A 7-filter ffmpeg filter chain applied before transcoding to maximise speech
recognition accuracy (<code>AudioPreprocessor</code> class):</p>
<table>
<thead>
<tr>
<th>#</th>
<th>Filter</th>
<th>Default</th>
<th>ffmpeg Filter</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>High-pass (80 Hz)</td>
<td>On</td>
<td><code>highpass=f=80</code></td>
<td>Remove low-frequency rumble</td>
</tr>
<tr>
<td>2</td>
<td>Low-pass (8 kHz)</td>
<td>On</td>
<td><code>lowpass=f=8000</code></td>
<td>Cut high-frequency hiss</td>
</tr>
<tr>
<td>3</td>
<td>Noise gate (-40 dB)</td>
<td>On</td>
<td><code>agate</code></td>
<td>Suppress background noise</td>
</tr>
<tr>
<td>4</td>
<td>De-esser (5 kHz)</td>
<td>Off</td>
<td><code>equalizer</code></td>
<td>Reduce sibilance</td>
</tr>
<tr>
<td>5</td>
<td>Dynamic range compressor</td>
<td>On</td>
<td><code>acompressor</code></td>
<td>Even out volume levels</td>
</tr>
<tr>
<td>6</td>
<td>Loudness normalisation</td>
<td>On</td>
<td><code>loudnorm</code> (EBU R128)</td>
<td>Standardise to -16 LUFS</td>
</tr>
<tr>
<td>7</td>
<td>Silence trimming</td>
<td>On</td>
<td><code>silenceremove</code></td>
<td>Remove leading/trailing silence</td>
</tr>
</tbody>
</table>
<p>All filter parameters (frequency, threshold, ratio, attack, release) are
user-configurable via the Settings, Audio Processing tab (Advanced Mode only).
The preprocessor is skipped if ffmpeg is not available — the transcoder handles
the fallback path.</p>
<hr>
<h2 id="6-audio-format-support">6. Audio Format Support</h2>
<p>12 input formats detected by file extension:</p>
<div class="highlight"><pre><span></span><code>MP3, WAV, OGG, Opus, FLAC, M4A, AAC, WebM, WMA, AIFF, AMR, MP4
</code></pre></div>

<p>All files are transcoded to 16 kHz mono WAV (<code>pcm_s16le</code>) before being sent to
the provider for consistent results across all engines.</p>
<hr>
<h2 id="7-batch-folder-processing">7. Batch &amp; Folder Processing</h2>
<ul>
<li><strong>File, Add Files</strong> (Ctrl+O): Multi-select file dialog with audio-type filter.
  Opens the <strong>AddFileWizard</strong> for per-file configuration (provider, model,
  language, custom name, AI Action selection). The wizard includes an <strong>Audio
  Preview</strong> tool for single-file imports, with pitch-preserving speed control
  and optional clip-range selection.</li>
<li><strong>File, Add Folder</strong> (Ctrl+Shift+O): Recursively scans a folder for supported
  audio files. Shows cost estimation for cloud providers with a confirmation
  dialog before processing begins.</li>
<li><strong>Concurrent workers</strong>: Configurable (default: 2). Each worker picks from a
  shared <code>queue.Queue</code> and runs preprocess, transcode, then transcribe.</li>
<li><strong>Limits</strong> (configurable in Advanced Settings):</li>
<li>Max file size: 500 MB</li>
<li>Max duration: 4 hours</li>
<li>Max batch files: 100</li>
<li>Max batch size: 10 GB</li>
<li>Chunk duration: 30 min (with 2 s overlap)</li>
<li><strong>Pause / Resume</strong> (F6): Pauses the queue; active jobs continue.</li>
<li><strong>Cancel Selected</strong> (Delete): Cancels a single job.</li>
<li><strong>Clear Queue</strong> (Ctrl+Shift+Del): Removes all unstarted jobs.</li>
<li><strong>Clear Completed</strong>: Removes finished jobs from the queue (Queue menu or
  toolbar).</li>
<li><strong>Retry All Failed</strong>: Re-queues all failed jobs (Queue menu or toolbar).</li>
<li><strong>Retry Selected</strong>: Re-queues a single failed or cancelled job (context menu).</li>
<li><strong>Rename</strong> (F2): Rename the selected file or folder with a custom display
  name.</li>
</ul>
<h3 id="budget-limits">Budget Limits</h3>
<p><code>BudgetSettings</code> (in <code>core/settings.py</code>) provides per-provider and per-model
spending limits for cloud transcription:</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>enabled</code></td>
<td>bool</td>
<td>True</td>
<td>Enable budget tracking</td>
</tr>
<tr>
<td><code>default_limit</code></td>
<td>float</td>
<td>0.0</td>
<td>Default per-batch limit (0 = unlimited)</td>
</tr>
<tr>
<td><code>provider_limits</code></td>
<td>dict</td>
<td>{}</td>
<td>Per-provider or per-model limits</td>
</tr>
<tr>
<td><code>always_confirm_paid</code></td>
<td>bool</td>
<td>True</td>
<td>Show confirmation for paid providers</td>
</tr>
</tbody>
</table>
<p>Budget limits are checked before batch processing starts. If estimated cost
exceeds the configured limit, a confirmation dialog is shown. Provider-specific
and model-specific limits override the default.</p>
<h3 id="cost-estimation">Cost Estimation</h3>
<p>Cloud provider costs are estimated before transcription begins using per-minute
rates from the provider table. The <code>AddFileWizard</code> and folder import dialog
display estimated costs. Cost estimation uses audio duration (detected via
pydub/ffprobe) with a file-size-based fallback for files that can't be probed.</p>
<h3 id="queue-panel-uiqueue_panelpy">Queue Panel (<code>ui/queue_panel.py</code>)</h3>
<p>The queue panel uses a <code>wx.TreeCtrl</code> with hierarchical folder grouping:</p>
<ul>
<li><strong>TreeView layout</strong>: Files are grouped under collapsible folder nodes when
  added from directories. Root-level files appear at the top level.</li>
<li><strong>Folder nodes</strong>: Show folder name, file count, and aggregated status summary
  (e.g., "3 files — 2 completed, 1 pending")</li>
<li><strong>Item display</strong>: Each file shows display name, provider, status, cost
  estimate, progress percentage, and AI Action indicators</li>
<li><strong>AI Action indicators</strong>: ⭐ (pending action), ⏳ (running), ✓ (completed), ✗
  (failed)</li>
<li><strong>Toolbar</strong>: Filter/search bar, Clear Completed button, Retry All Failed
  button</li>
<li><strong>Filter/search</strong>: Real-time text filter above the tree; matches file name,
  custom name, provider, status, and folder name. Case-insensitive.</li>
<li><strong>Context menus</strong>: Right-click files for Rename, Retry, Cancel, Remove, Change
  Provider/Model/Language, AI Action, Properties; right-click folders for
  Rename, Set AI Action for Pending, Remove Folder, Expand/Collapse</li>
<li><strong>Drag and drop</strong>: Drop audio files directly onto the queue panel</li>
<li><strong>Keyboard shortcuts</strong>: F2 (Rename), Delete (Cancel/Remove), F5 (Start)</li>
</ul>
<h3 id="custom-job-naming">Custom Job Naming</h3>
<p>Jobs and folders support custom display names:</p>
<ul>
<li><code>Job.custom_name: str</code> — User-assigned name for a transcription job</li>
<li><code>Job.display_name</code> property — Returns <code>custom_name</code> if set, else <code>file_name</code>,
  else derived from <code>file_path</code></li>
<li>Renamed via F2 keyboard shortcut, context menu, or inline during AddFileWizard</li>
<li>Custom names appear in the queue panel, transcript panel, exports, and
  Properties dialog</li>
<li>Folder custom names are tracked separately in
  <code>QueuePanel._folder_custom_names</code></li>
</ul>
<hr>
<h2 id="8-background-processing-system-tray">8. Background Processing &amp; System Tray</h2>
<h3 id="system-tray-icon-uitray_iconpy">System Tray Icon (<code>ui/tray_icon.py</code>)</h3>
<p>A <code>wx.adv.TaskBarIcon</code> that provides background-mode support:</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Tray icon</strong></td>
<td>Programmatic 16x16 "B" icon; always present when app is running</td>
</tr>
<tr>
<td><strong>Tooltip progress</strong></td>
<td>Shows "Transcribing X/Y (Z%)" or "Idle" on hover</td>
</tr>
<tr>
<td><strong>Left-click</strong></td>
<td>Toggle window visibility (show/hide)</td>
</tr>
<tr>
<td><strong>Right-click menu</strong></td>
<td>Show/Hide • Pause/Resume • Progress summary • Quit</td>
</tr>
<tr>
<td><strong>Balloon notifications</strong></td>
<td>Job complete, batch complete, and error notifications</td>
</tr>
</tbody>
</table>
<h3 id="minimize-to-tray">Minimize to Tray</h3>
<ul>
<li><strong>View, Minimize to System Tray</strong> (default: on): When enabled, closing the
  window hides it to the tray instead of quitting. Processing continues.</li>
<li>The <strong>close</strong> button and <strong>Alt+F4</strong> hide the window; the tray context menu's
  <strong>Quit</strong> item is the true exit.</li>
<li>When a batch completes while minimised, the app restores itself automatically.</li>
<li><code>EVT_ICONIZE</code> also hides to tray on minimize.</li>
</ul>
<h3 id="notifications">Notifications</h3>
<ul>
<li><strong>Balloon/toast</strong>: Job completion, batch completion, and error notifications
  appear via <code>ShowBalloon()</code> when the window is hidden.</li>
<li>A <strong>system bell</strong> (<code>wx.Bell()</code>) sounds on batch completion.</li>
<li>Notification settings (enable/disable, sound on/off) configurable in Settings,
  General, Behaviour section.</li>
</ul>
<hr>
<h2 id="9-output-export">9. Output &amp; Export</h2>
<h3 id="7-export-formats">7 Export Formats</h3>
<table>
<thead>
<tr>
<th>Format</th>
<th>Module</th>
<th>Extension</th>
<th>Timestamps</th>
<th>Diarization</th>
</tr>
</thead>
<tbody>
<tr>
<td>Plain Text</td>
<td><code>plain_text.py</code></td>
<td><code>.txt</code></td>
<td>Optional</td>
<td>Optional</td>
</tr>
<tr>
<td>Markdown</td>
<td><code>markdown.py</code></td>
<td><code>.md</code></td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>HTML</td>
<td><code>html_export.py</code></td>
<td><code>.html</code></td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Word</td>
<td><code>word_export.py</code></td>
<td><code>.docx</code></td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>SRT</td>
<td><code>srt.py</code></td>
<td><code>.srt</code></td>
<td>Required</td>
<td>No</td>
</tr>
<tr>
<td>VTT</td>
<td><code>vtt.py</code></td>
<td><code>.vtt</code></td>
<td>Required</td>
<td>No</td>
</tr>
<tr>
<td>JSON</td>
<td><code>json_export.py</code></td>
<td><code>.json</code></td>
<td>Yes</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<h3 id="auto-export-on-completion">Auto-Export on Completion</h3>
<p>When enabled (<strong>View, Auto-Export on Completion</strong>), each completed transcript is
automatically saved as a <code>.txt</code> file alongside the source audio file. If a file
with the same name exists, a numeric suffix is appended (e.g.,
<code>interview_1.txt</code>, <code>interview_2.txt</code>).</p>
<hr>
<h2 id="10-user-interface">10. User Interface</h2>
<h3 id="layout">Layout</h3>
<div class="highlight"><pre><span></span><code>Layout:
  Top:      Menu Bar (File, Queue, View, Tools, Help)
  Left:     Queue Panel (file list)
  Right:    Transcript Panel (viewer / editor)
  Bottom:   Status Bar [status] [progress gauge] [hw]
</code></pre></div>

<h3 id="menu-structure">Menu Structure</h3>
<h4 id="file">File</h4>
<ul>
<li>Add Files… (Ctrl+O)</li>
<li>Add Folder… (Ctrl+Shift+O)</li>
<li>Recent Files (numbered list, Clear Recent Files)</li>
<li>Export Transcript… (Ctrl+E)</li>
<li>Exit (Alt+F4)</li>
</ul>
<h4 id="queue">Queue</h4>
<ul>
<li>Start Transcription (F5)</li>
<li>Pause (F6)</li>
<li>Cancel Selected (Delete)</li>
<li>Clear Queue (Ctrl+Shift+Del)</li>
</ul>
<h4 id="view">View</h4>
<ul>
<li>Advanced Mode (Ctrl+Shift+A) — toggle check item</li>
<li>Minimize to System Tray — toggle check item (default: on)</li>
<li>Auto-Export on Completion — toggle check item (default: off)</li>
</ul>
<h4 id="tools">Tools</h4>
<ul>
<li>Settings… (Ctrl+,)</li>
<li>Manage Models… (Ctrl+M)</li>
<li>Audio Preview… (Ctrl+Shift+P)</li>
<li>Add Provider…</li>
<li>Copilot Setup…</li>
<li>Hardware Info…</li>
<li>View Log…</li>
</ul>
<h4 id="ai">AI</h4>
<ul>
<li>Translate (Ctrl+T)</li>
<li>Summarize (Ctrl+Shift+S)</li>
<li>Copilot Chat (Ctrl+Shift+C)</li>
<li>AI Action Builder…</li>
<li>AI Provider Settings…</li>
</ul>
<h4 id="help">Help</h4>
<ul>
<li>Setup Wizard…</li>
<li>Check for Updates…</li>
<li>Learn more about BITS</li>
<li>About… (F1)</li>
</ul>
<h3 id="settings-dialog-9-tabs">Settings Dialog (9 tabs)</h3>
<table>
<thead>
<tr>
<th>Tab</th>
<th>Visibility</th>
<th>Contents</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>General</strong></td>
<td>Always</td>
<td>Provider selection, language, timestamps, diarization, Behaviour section (minimize-to-tray, auto-export, notifications, sound)</td>
</tr>
<tr>
<td><strong>Transcription</strong></td>
<td>Always</td>
<td>Timestamps, speakers, confidence, word-level, segmentation, VAD, temperature, beam size, compute type</td>
</tr>
<tr>
<td><strong>Output</strong></td>
<td>Always</td>
<td>Default export format, output directory, filename template, encoding</td>
</tr>
<tr>
<td><strong>Playback</strong></td>
<td>Always</td>
<td>Audio preview speed range, step size, and jump timing</td>
</tr>
<tr>
<td><strong>Providers &amp; Keys</strong></td>
<td>Always</td>
<td>API key entry per provider with Test button validation</td>
</tr>
<tr>
<td><strong>Paths &amp; Storage</strong></td>
<td>Always</td>
<td>Output dir, models dir, temp dir, log file</td>
</tr>
<tr>
<td><strong>AI Providers</strong></td>
<td>Always</td>
<td>AI provider selection (5 providers), model selection, temperature, max tokens, translation language, summarization style</td>
</tr>
<tr>
<td><strong>Audio Processing</strong></td>
<td>Advanced Mode</td>
<td>All 7 preprocessing filter toggles &amp; parameters</td>
</tr>
<tr>
<td><strong>Advanced</strong></td>
<td>Advanced Mode</td>
<td>Max file size, duration, batch limits, concurrency, chunking, GPU, log level</td>
</tr>
</tbody>
</table>
<h3 id="simple-vs-advanced-mode">Simple vs Advanced Mode</h3>
<ul>
<li><strong>Basic Mode</strong> (default): Shows General, Transcription, Output, Providers &amp;
  Keys, Paths &amp; Storage, and AI Providers tabs. Audio Processing and Advanced
  tabs are hidden. Only local providers and <strong>activated</strong> cloud providers appear
  in the provider dropdown. Cloud providers must be activated via the Add
  Provider wizard before they become available. Sensible defaults are applied
  automatically.</li>
<li><strong>Advanced Mode</strong> (Ctrl+Shift+A): Reveals all 8 settings tabs. All cloud
  providers appear in the provider dropdown regardless of activation status.
  Full control over audio preprocessing, GPU settings, concurrency, and chunking
  parameters.</li>
<li><strong>Experience Mode Setting</strong>: Persisted in <code>settings.json</code> as
  <code>general.experience_mode</code> ("basic" or "advanced"). Set during the Setup Wizard
  or toggled via View, then Advanced Mode.</li>
</ul>
<h3 id="recent-files">Recent Files</h3>
<ul>
<li>Persisted to <code>DATA_DIR/recent_files.json</code> (max 10 entries).</li>
<li>Accessible via File, Recent Files submenu with numbered mnemonics (&amp;1 ...).</li>
<li>Clear Recent Files option at the bottom.</li>
<li>Non-existent files are silently removed when selected.</li>
</ul>
<hr>
<h2 id="11-self-update-system">11. Self-Update System</h2>
<p><code>core/updater.py</code> implements GitHub Releases-based update checking:</p>
<ol>
<li><strong>Startup check</strong>: Silent background check 3 seconds after launch. If a newer
   version exists, the status bar shows a notification.</li>
<li><strong>Manual check</strong>: Help, Check for Updates opens a dialog with version
   comparison and a link to the release page.</li>
<li><strong>Version comparison</strong> uses <code>packaging.version.Version</code> for correct semver.</li>
<li><strong>No auto-install</strong> — the user downloads the new version manually.</li>
</ol>
<hr>
<h2 id="12-architecture">12. Architecture</h2>
<h3 id="file-tree">File Tree</h3>
<div class="highlight"><pre><span></span><code>src/bits_whisperer/
  __main__.py              # Entry point
  app.py                   # wx.App subclass
  core/
    transcription_service.py  # Job queue, workers, orchestration
    provider_manager.py       # Provider registry &amp; routing
    audio_preprocessor.py     # 7-filter ffmpeg preprocessing chain
    audio_player.py           # Audio preview playback (ffmpeg + sounddevice)
    dependency_checker.py     # Startup dependency verification &amp; install
    device_probe.py           # Hardware detection (CPU/RAM/GPU/CUDA)
    diarization.py            # Cloud-free local speaker diarization (pyannote)
    model_manager.py          # Whisper model download &amp; cache
    sdk_installer.py          # On-demand provider SDK installer
    wheel_installer.py        # PyPI wheel downloader/extractor (frozen builds)
    settings.py               # Persistent settings (JSON-backed dataclass)
    transcoder.py             # ffmpeg WAV normalisation
    updater.py                # GitHub Releases self-update
    job.py                    # Job / TranscriptionResult data models
    ai_service.py             # AI translation &amp; summarization (OpenAI/Anthropic/Azure/Gemini/Copilot)
    live_transcription.py     # Real-time microphone transcription
    plugin_manager.py         # Plugin discovery, loading &amp; lifecycle
    copilot_service.py        # GitHub Copilot SDK integration &amp; agent management
    context_manager.py        # Context window management &amp; token budgeting
    document_reader.py        # Document text extraction (DOCX/PDF/XLSX/RTF/TXT)
  providers/                    # 17 provider adapters (strategy pattern)
    base.py                   # TranscriptionProvider ABC + ProviderCapabilities
    local_whisper.py          # faster-whisper (local, free)
    openai_whisper.py         # OpenAI Whisper API
    google_speech.py          # Google Cloud Speech-to-Text
    gemini_provider.py        # Google Gemini
    azure_speech.py           # Microsoft Azure Speech Services
    azure_embedded.py         # Microsoft Azure Embedded Speech (offline)
    aws_transcribe.py         # Amazon Transcribe
    deepgram_provider.py      # Deepgram Nova-2
    assemblyai_provider.py    # AssemblyAI
    groq_whisper.py           # Groq LPU Whisper
    rev_ai_provider.py        # Rev.ai
    speechmatics_provider.py  # Speechmatics
    elevenlabs_provider.py    # ElevenLabs Scribe
    windows_speech.py         # Windows SAPI5 + WinRT (offline)
    vosk_provider.py          # Vosk offline speech (Kaldi-based)
    parakeet_provider.py      # NVIDIA Parakeet (NeMo ASR, English)
    auphonic_provider.py      # Auphonic audio post-production + transcription
  export/                       # Output formatters
    base.py                   # ExportFormatter ABC
    plain_text.py, markdown.py, html_export.py
    word_export.py, srt.py, vtt.py, json_export.py
  storage/
    database.py               # SQLite (WAL mode) for job metadata
    key_store.py              # keyring-backed credential store
  ui/
    main_frame.py             # Menu bar, splitter, status bar, tray integration
    queue_panel.py            # File queue list
    transcript_panel.py       # Transcript viewer / editor with speaker management
    settings_dialog.py        # Tabbed settings (9 tabs)
    progress_dialog.py        # Batch progress display
    model_manager_dialog.py   # Model download &amp; management
    add_provider_dialog.py    # Cloud provider onboarding wizard
    setup_wizard.py           # First-run setup wizard (8 pages)
    tray_icon.py              # System tray (TaskBarIcon)
    live_transcription_dialog.py  # Live microphone transcription dialog
    ai_settings_dialog.py     # AI provider configuration dialog (5 providers)
    copilot_setup_dialog.py   # Copilot CLI installation &amp; auth wizard
    copilot_chat_panel.py     # Interactive AI transcript chat panel
    agent_builder_dialog.py   # AI Action Builder — post-transcription template editor
    audio_player_dialog.py   # Audio preview dialog with clip selection
  utils/
    accessibility.py          # a11y helpers (announce, set_name, safe_call_after)
    constants.py              # App constants, model registry, path definitions
    platform_utils.py         # Cross-platform helpers (file open, disk space, CPU/GPU detection)
</code></pre></div>

<h3 id="threading-model">Threading Model</h3>
<ul>
<li><strong>Main thread</strong>: wxPython event loop and all UI operations.</li>
<li><strong>Worker threads</strong>: <code>TranscriptionService</code> spawns N daemon threads (default 2)
  that process jobs from a <code>queue.Queue</code>.</li>
<li><strong>Thread safety</strong>: All UI updates go through <code>wx.CallAfter()</code> via
  <code>safe_call_after()</code>. The service uses a <code>threading.Lock</code> for shared state.</li>
<li><strong>Update checker</strong>: Runs in a separate daemon thread (startup + manual).</li>
</ul>
<h3 id="shutdown-temp-file-cleanup">Shutdown &amp; Temp File Cleanup</h3>
<p>BITS Whisperer implements a robust ordered shutdown sequence to prevent resource
leaks and orphaned temporary files:</p>
<ol>
<li><strong>Ordered shutdown</strong> (<code>MainFrame._on_close</code>): 5-step sequence — (1) stop
   transcription service, (2) stop Copilot service, (3) save settings, (4)
   cleanup tray icon, (5) remove stale temp files.</li>
<li><strong>Worker thread joining</strong>: <code>TranscriptionService.stop()</code> joins worker threads
   with a 5-second timeout instead of fire-and-forget sentinel values.</li>
<li><strong>Per-job temp file tracking</strong>: <code>TranscriptionService</code> tracks all temp files
   created by the preprocessor and transcoder during each job and cleans them up
   on job completion and on service shutdown.</li>
<li><strong>Identifiable temp file prefixes</strong>: Transcoder uses <code>bw_transcode_*</code>,
   preprocessor uses <code>bw_preprocess_*</code>, and the updater uses <code>bw_update_*</code>
   prefixes to enable targeted cleanup.</li>
<li><strong>Stale temp file cleanup</strong>: <code>MainFrame._cleanup_stale_temp_files()</code> scans
   the system temp directory on shutdown for files/directories older than 1 hour
   matching <code>bw_transcode_*</code>, <code>bw_preprocess_*</code>, or <code>bw_update_*</code> prefixes.</li>
<li><strong>Safety nets</strong>: <code>App.OnExit()</code> and an <code>atexit</code> handler in <code>__main__.py</code>
   provide last-resort temp file cleanup if the normal shutdown path is
   bypassed.</li>
<li><strong>Secure temp file creation</strong>: Transcoder uses <code>tempfile.mkstemp()</code> instead
   of the deprecated <code>tempfile.mktemp()</code> to avoid race conditions.</li>
</ol>
<hr>
<h2 id="13-data-model">13. Data Model</h2>
<h3 id="job-corejobpy">Job (<code>core/job.py</code>)</h3>
<div class="highlight"><pre><span></span><code>Job
  - id: str (UUID)
  - file_path, file_name, file_size_bytes
  - duration_seconds: float
  - status: JobStatus (PENDING to TRANSCODING to TRANSCRIBING to COMPLETED | FAILED | CANCELLED)
  - provider, model, language
  - created_at, started_at, completed_at: ISO timestamps
  - progress_percent: 0-100
  - cost_estimate, cost_actual: float (USD)
  - transcript_path: str
  - error_message: str
  - include_timestamps, include_diarization: bool
  - clip_start_seconds, clip_end_seconds: float | None (optional clip range)
  - custom_name: str (user-defined display name)
  - ai_action_template: str (built-in preset name or AgentConfig JSON path)
  - ai_action_result: str (AI processing output)
  - ai_action_status: str (&quot;&quot; | &quot;running&quot; | &quot;completed&quot; | &quot;failed&quot;)
  - ai_action_error: str (error message if AI action failed)
  - ai_action_attachments: list[dict] (per-job attachment overrides)
  - result: TranscriptionResult | None
</code></pre></div>

<h3 id="transcriptionresult">TranscriptionResult</h3>
<div class="highlight"><pre><span></span><code>TranscriptionResult
  - job_id, audio_file, provider, model, language
  - duration_seconds
  - segments: list[TranscriptSegment]
      - start, end: float (seconds)
      - text: str
      - confidence: float (0-1)
      - speaker: str
  - full_text: str
  - speaker_map: dict[str, str]  # internal ID -&gt; display name
  - created_at: str
</code></pre></div>

<h3 id="persistence">Persistence</h3>
<table>
<thead>
<tr>
<th>Store</th>
<th>Backend</th>
<th>Contents</th>
</tr>
</thead>
<tbody>
<tr>
<td>Job metadata</td>
<td>SQLite (WAL mode)</td>
<td>Job history, status, paths</td>
</tr>
<tr>
<td>API keys</td>
<td>keyring (Credential Mgr)</td>
<td>Provider API keys (20 entries)</td>
</tr>
<tr>
<td>Recent files</td>
<td>JSON file</td>
<td>Last 10 opened file paths</td>
</tr>
<tr>
<td>Whisper models</td>
<td>File system (models dir)</td>
<td>Downloaded faster-whisper models</td>
</tr>
<tr>
<td>Provider SDKs</td>
<td>File system (site-packages)</td>
<td>On-demand installed Python packages</td>
</tr>
<tr>
<td>Transcripts</td>
<td>File system (transcripts)</td>
<td>Default export location</td>
</tr>
<tr>
<td>App log</td>
<td>File system</td>
<td>Rotating log at <code>DATA_DIR/app.log</code></td>
</tr>
</tbody>
</table>
<hr>
<h2 id="14-privacy-security">14. Privacy &amp; Security</h2>
<ul>
<li><strong>Local by default</strong>: Transcripts stored in user data directory (Windows:
  <code>%LOCALAPPDATA%/BITS Whisperer/</code>, macOS:
  <code>~/Library/Application Support/BITS Whisperer/</code>).</li>
<li><strong>API keys</strong>: Stored in OS credential vault (Windows Credential Manager /
  macOS Keychain) via <code>keyring</code> — never logged, printed, or committed.</li>
<li><strong>Key validation</strong>: Dry-run API call on save to verify keys are working.</li>
<li><strong>No telemetry</strong>: The app sends no usage data. Update checks are opt-in REST
  calls to the GitHub Releases API.</li>
<li><strong>Offline-capable</strong>: 5 local providers (Local Whisper, Windows SAPI5/WinRT,
  Azure Embedded Speech, Vosk, Parakeet) work without any internet connection.</li>
</ul>
<hr>
<h2 id="15-accessibility-requirements-non-negotiable">15. Accessibility Requirements (Non-Negotiable)</h2>
<p>Adapted from WCAG 2.1/2.2 for desktop; detailed rules in
<code>.github/accessibility.agent.md</code>.</p>
<table>
<thead>
<tr>
<th>#</th>
<th>Requirement</th>
<th>Implementation</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Every control has an accessible name</td>
<td><code>SetName()</code> on all widgets</td>
</tr>
<tr>
<td>2</td>
<td>Label association for all inputs</td>
<td><code>wx.StaticText</code> + <code>SetName()</code> pairing</td>
</tr>
<tr>
<td>3</td>
<td>Full keyboard reachability</td>
<td>Tab order, mnemonics, accelerators</td>
</tr>
<tr>
<td>4</td>
<td>All actions available from menu bar</td>
<td>Mnemonics (&amp;) + accelerator keys</td>
</tr>
<tr>
<td>5</td>
<td>Progress reporting for screen readers</td>
<td><code>wx.Gauge</code> + status bar text updates</td>
</tr>
<tr>
<td>6</td>
<td>Cross-thread UI safety</td>
<td><code>wx.CallAfter()</code> / <code>safe_call_after()</code></td>
</tr>
<tr>
<td>7</td>
<td>High contrast support</td>
<td><code>wx.SystemSettings.GetColour()</code>, no hard-coded colours</td>
</tr>
<tr>
<td>8</td>
<td>Screen reader compatibility</td>
<td>Tested with NVDA keyboard-only</td>
</tr>
<tr>
<td>9</td>
<td>Focus management on dialog open/close</td>
<td>Focus set to first interactive control</td>
</tr>
<tr>
<td>10</td>
<td>Status announcements for state changes</td>
<td><code>announce_status()</code> helper</td>
</tr>
<tr>
<td>11</td>
<td>Consistent navigation patterns</td>
<td>Splitter, Tab, List, Action pattern</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="16-keyboard-shortcuts">16. Keyboard Shortcuts</h2>
<table>
<thead>
<tr>
<th>Action</th>
<th>Shortcut</th>
<th>Context</th>
</tr>
</thead>
<tbody>
<tr>
<td>Add Files</td>
<td>Ctrl+O</td>
<td>Global</td>
</tr>
<tr>
<td>Add Folder</td>
<td>Ctrl+Shift+O</td>
<td>Global</td>
</tr>
<tr>
<td>Export Transcript</td>
<td>Ctrl+E</td>
<td>Global</td>
</tr>
<tr>
<td>Find Next</td>
<td>F3</td>
<td>Transcript</td>
</tr>
<tr>
<td>Start Transcription</td>
<td>F5</td>
<td>Global</td>
</tr>
<tr>
<td>Pause / Resume</td>
<td>F6</td>
<td>Global</td>
</tr>
<tr>
<td>Cancel Selected</td>
<td>Delete</td>
<td>Queue</td>
</tr>
<tr>
<td>Clear Queue</td>
<td>Ctrl+Shift+Del</td>
<td>Queue</td>
</tr>
<tr>
<td>Settings</td>
<td>Ctrl+,</td>
<td>Global</td>
</tr>
<tr>
<td>Manage Models</td>
<td>Ctrl+M</td>
<td>Global</td>
</tr>
<tr>
<td>Toggle Advanced Mode</td>
<td>Ctrl+Shift+A</td>
<td>Global</td>
</tr>
<tr>
<td>Audio Preview</td>
<td>Ctrl+Shift+P</td>
<td>Global</td>
</tr>
<tr>
<td>Preview Selected (Queue)</td>
<td>Ctrl+Alt+P</td>
<td>Queue</td>
</tr>
<tr>
<td>Copilot Chat</td>
<td>Ctrl+Shift+C</td>
<td>Global</td>
</tr>
<tr>
<td>About</td>
<td>F1</td>
<td>Global</td>
</tr>
<tr>
<td>Exit (or minimize to tray)</td>
<td>Alt+F4</td>
<td>Global</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="17-paths-directories">17. Paths &amp; Directories</h2>
<p>All user data is stored under <code>%LOCALAPPDATA%/BITS Whisperer/BITSWhisperer/</code>
(via <code>platformdirs.user_data_dir</code>):</p>
<table>
<thead>
<tr>
<th>Path</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>&lt;DATA_DIR&gt;/</code></td>
<td>Application data root</td>
</tr>
<tr>
<td><code>transcripts/</code></td>
<td>Default transcript export location</td>
</tr>
<tr>
<td><code>models/</code></td>
<td>Downloaded Whisper model files</td>
</tr>
<tr>
<td><code>site-packages/</code></td>
<td>On-demand installed provider SDKs</td>
</tr>
<tr>
<td><code>bits_whisperer.db</code></td>
<td>SQLite job database</td>
</tr>
<tr>
<td><code>app.log</code></td>
<td>Application log file</td>
</tr>
<tr>
<td><code>recent_files.json</code></td>
<td>Recent file history</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="18-dependencies">18. Dependencies</h2>
<p>From <code>pyproject.toml</code>:</p>
<table>
<thead>
<tr>
<th>Package</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td>wxPython ≥ 4.2.0</td>
<td>Desktop UI framework</td>
</tr>
<tr>
<td>faster-whisper ≥ 1.0.0</td>
<td>Local Whisper inference</td>
</tr>
<tr>
<td>openai ≥ 1.0.0</td>
<td>OpenAI Whisper API</td>
</tr>
<tr>
<td>google-cloud-speech ≥ 2.20.0</td>
<td>Google Speech-to-Text</td>
</tr>
<tr>
<td>azure-cognitiveservices-speech ≥ 1.32.0</td>
<td>Azure Speech Services</td>
</tr>
<tr>
<td>deepgram-sdk ≥ 3.0.0</td>
<td>Deepgram Nova-2</td>
</tr>
<tr>
<td>assemblyai ≥ 0.20.0</td>
<td>AssemblyAI</td>
</tr>
<tr>
<td>boto3 ≥ 1.28.0</td>
<td>Amazon Transcribe (AWS)</td>
</tr>
<tr>
<td>google-genai ≥ 0.4.0</td>
<td>Google Gemini</td>
</tr>
<tr>
<td>groq ≥ 0.4.0</td>
<td>Groq LPU Whisper</td>
</tr>
<tr>
<td>rev-ai ≥ 2.17.0</td>
<td>Rev.ai</td>
</tr>
<tr>
<td>speechmatics-python ≥ 1.0.0</td>
<td>Speechmatics</td>
</tr>
<tr>
<td>keyring ≥ 24.0.0</td>
<td>OS credential store</td>
</tr>
<tr>
<td>python-docx ≥ 1.0.0</td>
<td>Word export</td>
</tr>
<tr>
<td>markdown ≥ 3.5</td>
<td>Markdown rendering</td>
</tr>
<tr>
<td>Jinja2 ≥ 3.1.0</td>
<td>HTML template export</td>
</tr>
<tr>
<td>pydub ≥ 0.25.1</td>
<td>Audio duration detection</td>
</tr>
<tr>
<td>psutil ≥ 5.9.0</td>
<td>System resource monitoring</td>
</tr>
<tr>
<td>platformdirs ≥ 4.0.0</td>
<td>Cross-platform data dirs</td>
</tr>
<tr>
<td>httpx ≥ 0.25.0</td>
<td>HTTP client (update checker)</td>
</tr>
<tr>
<td>packaging ≥ 23.0</td>
<td>Version comparison</td>
</tr>
<tr>
<td>sounddevice ≥ 0.4.6</td>
<td>Audio preview playback</td>
</tr>
<tr>
<td>winsdk ≥ 1.0.0b10</td>
<td>Windows Speech Runtime (Win)</td>
</tr>
<tr>
<td>comtypes ≥ 1.2.0</td>
<td>COM interop (Win)</td>
</tr>
</tbody>
</table>
<p>Dev dependencies: pytest, pytest-cov, black, ruff, mypy.</p>
<hr>
<h2 id="19-implementation-status">19. Implementation Status</h2>
<ul>
<li>[x] Core transcription pipeline (preprocess, transcode, transcribe)</li>
<li>[x] 17 transcription provider adapters (including Auphonic, Vosk, Parakeet)</li>
<li>[x] 14 Whisper model definitions with hardware eligibility</li>
<li>[x] 7-filter audio preprocessing with ffmpeg</li>
<li>[x] 7 export format adapters</li>
<li>[x] WXPython main frame with splitter layout</li>
<li>[x] Queue panel with file list</li>
<li>[x] Transcript panel with viewer/editor and speaker management</li>
<li>[x] Settings dialog (7 tabs, Basic/Advanced visibility)</li>
<li>[x] Model Manager dialog with download &amp; eligibility</li>
<li>[x] Hardware detection (CPU, RAM, GPU, CUDA)</li>
<li>[x] API key storage via keyring</li>
<li>[x] Cloud provider onboarding (Add Provider wizard with live validation)</li>
<li>[x] Provider-specific settings (per-provider configuration during onboarding)</li>
<li>[x] Speaker diarization (10 cloud providers + cloud-free pyannote.audio)</li>
<li>[x] Speaker editing UI (rename, reassign, create speakers post-transcription)</li>
<li>[x] Full Auphonic API integration (all audio algorithms, speech services,
  output formats)</li>
<li>[x] SQLite database for job metadata</li>
<li>[x] Batch processing with concurrent workers</li>
<li>[x] Progress reporting (gauge + status bar + screen reader)</li>
<li>[x] System tray icon with progress tooltip</li>
<li>[x] Balloon notifications (job complete, batch complete, errors)</li>
<li>[x] Minimize to tray / background processing</li>
<li>[x] Auto-export on completion</li>
<li>[x] Recent files menu (persistent, max 10)</li>
<li>[x] Self-update via GitHub Releases (startup + manual)</li>
<li>[x] Basic/Advanced mode toggle with persistent experience_mode setting</li>
<li>[x] View Log (opens app.log)</li>
<li>[x] Full accessibility (names, labels, keyboard, screen reader)</li>
<li>[x] First-run setup wizard (8-page guided experience with mode selection and
  AI/Copilot setup)</li>
<li>[x] Cross-platform support (Windows 10+ and macOS 12+)</li>
<li>[x] Disk space pre-checks before model downloads</li>
<li>[x] Comprehensive user guide (docs/USER_GUIDE.md)</li>
<li>[x] Automatic ffmpeg dependency installation (winget on Windows, manual
  instructions fallback)</li>
<li>[x] Persistent application settings (JSON-backed dataclass)</li>
<li>[x] On-demand provider SDK installer (WheelInstaller + sdk_installer)</li>
<li>[x] Pre-transcription SDK checks (ensure_sdk before job dispatch)</li>
<li>[x] Lightweight PyInstaller packaging (~40 MB with lean build)</li>
<li>[x] Inno Setup Windows installer script</li>
<li>[x] Queue panel context menu with wired handlers</li>
<li>[x] Find Next (F3) in transcript search</li>
<li>[x] Setup Wizard accessible from Help menu</li>
<li>[x] Learn more about BITS link in Help menu</li>
<li>[x] Add Provider menu item in Tools menu</li>
<li>[x] Provider activation tracking (activated_providers in settings)</li>
<li>[x] AI translation &amp; summarization (5 providers: OpenAI, Anthropic, Azure
  OpenAI, Gemini, Copilot)</li>
<li>[x] Google Gemini AI provider (translation, summarization)</li>
<li>[x] GitHub Copilot SDK integration (CopilotService, async client, streaming,
  custom tools)</li>
<li>[x] Interactive AI Chat Panel (Ctrl+Shift+C, streaming, quick actions,
  transcript context)</li>
<li>[x] Copilot Setup Wizard (4-step: CLI install, SDK install, auth, test)</li>
<li>[x] Agent Builder dialog (4-tab: Identity, Instructions with presets, Tools,
  Welcome Message)</li>
<li>[x] CopilotSettings dataclass (11 fields) in AppSettings</li>
<li>[x] Installer Copilot CLI install task (WinGet optional)</li>
<li>[x] 191 tests with full coverage for Gemini and Copilot features</li>
<li>[x] AI model catalog with real-time pricing (22 models across 4 providers)</li>
<li>[x] Copilot subscription tier-based model selection
  (Free/Pro/Business/Enterprise)</li>
<li>[x] Google Gemma models (5 variants: 27B, 12B, 4B, 1B, 3n-E4B via Gemini API)</li>
<li>[x] Custom vocabulary for AI translation/summarization accuracy</li>
<li>[x] 10 built-in prompt templates (4 translation, 4 summarization, 2 analysis)</li>
<li>[x] Multi-language simultaneous translation</li>
<li>[x] Real-time streaming from cloud providers (Deepgram, AssemblyAI)</li>
<li>[x] 255 tests with full coverage for all Phase 4 features</li>
<li>[x] TreeView queue panel with folder grouping and collapsible nodes</li>
<li>[x] Folder transcription with cost estimation and confirmation dialog</li>
<li>[x] Conditional Chat tab visibility based on AI provider configuration</li>
<li>[x] Queue panel toolbar with filter/search, Clear Completed, Retry All Failed</li>
<li>[x] Budget limits per provider and per model with confirmation dialogs</li>
<li>[x] Setup Wizard budget configuration page (9 pages total)</li>
<li>[x] Custom job naming (F2 rename, context menu, AddFileWizard)</li>
<li>[x] Job properties dialog (file details, status, cost, timestamps)</li>
<li>[x] Queue context menus (file: Rename/Retry/Cancel/Remove/Properties; folder:
  Rename/Remove/Expand)</li>
<li>[x] Batch operations (Clear Completed, Retry All Failed, Retry Selected)</li>
<li>[x] Real-time queue filter/search bar</li>
<li>[x] Ollama local AI provider (local LLM inference via OpenAI-compatible API)</li>
<li>[x] AI Actions — automatic post-transcription AI processing</li>
<li>[x] 6 built-in AI Action presets (Meeting Minutes, Action Items, Executive
  Summary, Interview Notes, Lecture Notes, Q&amp;A Extraction)</li>
<li>[x] AI Action Builder dialog (8 presets, 4-tab template editor)</li>
<li>[x] AI Action selection in AddFileWizard</li>
<li>[x] AI Action result display in Transcript Panel</li>
<li>[x] AI Action status indicators in Queue Panel (⭐/⏳/✓/✗)</li>
<li>[x] 501 tests with full coverage for all features</li>
<li>[x] Slash command system (28 commands, autocomplete, 2 categories)</li>
<li>[x] Context window management (model-aware token budgeting, 4 strategies,
  conversation trimming)</li>
<li>[x] 674 tests with full coverage for all features</li>
<li>[x] Robust shutdown procedures (ordered shutdown, worker thread joining, temp
  file cleanup, safety nets)</li>
<li>[x] Document attachments for AI Actions (attach reference documents — DOCX,
  PDF, XLSX, RTF, TXT — to provide additional context for AI processing)</li>
<li>[x] Audio preview with pitch-preserving speed control and clip selection</li>
</ul>
<hr>
<h2 id="20-success-metrics">20. Success Metrics</h2>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Target</th>
</tr>
</thead>
<tbody>
<tr>
<td>Time to first transcription</td>
<td>&lt; 2 minutes from install</td>
</tr>
<tr>
<td>Keyboard-only task completion</td>
<td>100% of features</td>
</tr>
<tr>
<td>Screen reader compatibility</td>
<td>NVDA pass on all workflows</td>
</tr>
<tr>
<td>Provider switch time</td>
<td>&lt; 30 seconds</td>
</tr>
<tr>
<td>Batch throughput (local)</td>
<td>Limited only by hardware</td>
</tr>
<tr>
<td>Cold start time</td>
<td>&lt; 5 seconds</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="21-decisions-log">21. Decisions Log</h2>
<table>
<thead>
<tr>
<th>Decision</th>
<th>Reasoning</th>
</tr>
</thead>
<tbody>
<tr>
<td>WXPython over Electron/Qt</td>
<td>Best native accessibility (MSAA/UIA) on Windows</td>
</tr>
<tr>
<td>faster-whisper over openai-whisper</td>
<td>CTranslate2 -- 4x faster, lower memory, same accuracy</td>
</tr>
<tr>
<td>Menu bar as primary interface</td>
<td>Screen reader–friendly; discoverable via mnemonics</td>
</tr>
<tr>
<td>keyring for API keys</td>
<td>OS credential store &gt; env vars or config files</td>
</tr>
<tr>
<td>Simple/Advanced mode toggle</td>
<td>Consumer-friendly defaults; power users opt in; mode persisted across sessions</td>
</tr>
<tr>
<td>Basic mode provider filtering</td>
<td>Only show activated cloud providers in Basic mode to reduce clutter</td>
</tr>
<tr>
<td>Cloud provider onboarding</td>
<td>Three-step wizard with live API validation before activation</td>
</tr>
<tr>
<td>Minimize to tray by default</td>
<td>Long batch jobs shouldn't block the taskbar</td>
</tr>
<tr>
<td>Balloon notifications</td>
<td>Native Windows toast API via wx.adv; no extra deps</td>
</tr>
<tr>
<td>JSON for recent files</td>
<td>Lightweight; no schema migration needed</td>
</tr>
<tr>
<td>No auto-install for updates</td>
<td>Security -- user controls what runs on their machine</td>
</tr>
<tr>
<td>On-demand SDK install</td>
<td>Keeps installer ~40 MB; SDKs downloaded on first use from PyPI</td>
</tr>
<tr>
<td>WheelInstaller over pip</td>
<td>Frozen apps have no Python interpreter; direct wheel extraction works everywhere</td>
</tr>
<tr>
<td>ffmpeg for preprocessing</td>
<td>Ubiquitous; no native lib compilation needed</td>
</tr>
<tr>
<td>pyannote.audio for local diarization</td>
<td>Best open-source speaker diarization; optional dependency</td>
</tr>
<tr>
<td>Provider configure() method</td>
<td>Data-driven settings injection; no provider subclass modification needed</td>
</tr>
<tr>
<td>SpeakerRenameDialog over inline</td>
<td>Global rename is safer and clearer than per-line editing</td>
</tr>
<tr>
<td>speaker_map on TranscriptionResult</td>
<td>Separates internal IDs from display names; lossless rename</td>
</tr>
<tr>
<td>Google Gemini for AI</td>
<td>Fast, affordable translation/summarization; multimodal capable</td>
</tr>
<tr>
<td>GitHub Copilot SDK over raw API</td>
<td>CLI-based auth, streaming, tool calling, session management built-in</td>
</tr>
<tr>
<td>Agent Builder as separate dialog</td>
<td>Complex config deserves dedicated UI; presets simplify setup</td>
</tr>
<tr>
<td>CopilotSettings as nested dataclass</td>
<td>Clean separation from AI settings; many Copilot-specific fields</td>
</tr>
<tr>
<td>TreeView for queue panel</td>
<td>Hierarchical folder grouping; native accessibility; collapsible nodes</td>
</tr>
<tr>
<td>Budget limits per provider+model</td>
<td>Granular cost control without blocking free/local providers</td>
</tr>
<tr>
<td>AI Actions via existing AgentConfig</td>
<td>Reuse existing template infrastructure; provider-agnostic design</td>
</tr>
<tr>
<td>Ollama via OpenAI-compatible API</td>
<td>No new SDK needed; uses existing openai package for API calls</td>
</tr>
<tr>
<td>Built-in presets for AI Actions</td>
<td>Common use cases work out of the box; no configuration required</td>
</tr>
</tbody>
</table>
<div class="footer">
  Generated from <code>PRD.md</code> &mdash; BITS Whisperer Documentation
</div>
</body>
</html>
